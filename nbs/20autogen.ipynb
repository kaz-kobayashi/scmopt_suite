{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d882fc47-e22e-4c3d-9910-5bfdbfcb4dc3",
   "metadata": {},
   "source": [
    "# SC Optimization を LLMでサポート\n",
    "\n",
    ">  Use OpenAI, LangChain and LM Studio to develop LLMs for SC Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547b312-7bbe-4484-98b7-cf4e3e773ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9eaa3d-8271-456c-9d62-0de10123e87a",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "- LM Studio: https://lmstudio.ai/  (download and install)  downlad \"mistralai_mistral-7b-instruct-v0.1 or 0.2\" and start server! See https://note.com/mikiokubo/n/nb61e5b558ef8 for more detail\n",
    "- install \"openai\" package and \"langchain\" (and new version \"langchain-openai\") package\n",
    "<!-- - autogen: https://microsoft.github.io/autogen/docs/Examples/\n",
    "- autogen-studioも同じgithubにある； agentにskillを追加可能\n",
    "- OptiGuide: https://github.com/microsoft/OptiGuide -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9d8f5-b3c8-4efd-b128-37c90a4776c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737c9b5-f589-48fb-8468-ec998019877e",
   "metadata": {},
   "source": [
    "## LangGraph\n",
    "\n",
    "何故か登録した関数を呼んでくれない．\n",
    "\n",
    "動かすにはLangSmith https://smith.langchain.com/ へ登録してAPIキーを環境変数に保管する必要がある．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900769e-4e25-4d6b-a739-f6d2f6c17287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import StructuredTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da13ab-09f1-4f46-acb9-9062b23fc46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiplierInput(BaseModel):\n",
    "#     a: int = Field(description=\"First number\")\n",
    "#     b: int = Field(description=\"Second number\")\n",
    "\n",
    "\n",
    "# def multiply(a: int, b: int) -> int:\n",
    "#     return a * b\n",
    "\n",
    "\n",
    "# multiplier = StructuredTool.from_function(\n",
    "#     func=multiply,\n",
    "#     name=\"Multiplier\",\n",
    "#     description=\"Multiply two numbers\",\n",
    "#     args_schema=MultiplierInput,\n",
    "#     return_direct=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f436bca-f100-43a0-8fff-5a8da1fe73bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AdderInput(BaseModel):\n",
    "#     a: int = Field(description=\"First number\")\n",
    "#     b: int = Field(description=\"Second number\")\n",
    "\n",
    "\n",
    "# def add(a: int, b: int) -> int:\n",
    "#     return a + b\n",
    "\n",
    "\n",
    "# adder = StructuredTool.from_function(\n",
    "#     func=add,\n",
    "#     name=\"Adder\",\n",
    "#     description=\"Add two numbers\",\n",
    "#     args_schema=AdderInput,\n",
    "#     return_direct=False,\n",
    "# )\n",
    "# tools = [multiplier, adder]\n",
    "# # Modification: we need to be able to execute tools from the graph\n",
    "# from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "# tool_executor = ToolExecutor(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a754db23-0006-4986-8bc8-c4b9237e76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "# class Response(BaseModel):\n",
    "#     \"\"\"Final answer to the user\"\"\"\n",
    "\n",
    "#     result: int = Field(description=\"the result of the computation\")\n",
    "#     explanation: str = Field(\n",
    "#         description=\"explanation of the steps taken to get the result\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ef403-897b-438d-aa7e-0b4bedecf9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `format_tool_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.2.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
      "  warn_deprecated(\n",
      "/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.2.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# # Modification: instead of initializing a full Agent Executor, we're just creating the model and banding functions to it\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.tools.render import format_tool_to_openai_function\n",
    "# from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
    "\n",
    "# # Create the OpenAI LLM\n",
    "# #llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0, streaming=True)\n",
    "# llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "# # Bind tools to the model\n",
    "# functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "# # Bind the resposne to the model\n",
    "# functions.append(convert_pydantic_to_openai_function(Response))\n",
    "\n",
    "# model = llm.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3d11c-b348-4f22-81cd-573dab9994a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import TypedDict, Annotated, Sequence\n",
    "# import operator\n",
    "# from langchain_core.messages import BaseMessage\n",
    "\n",
    "# class AgentState(TypedDict):\n",
    "#     messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f48002-0519-4061-90d1-7e5ec48dbbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.prebuilt import ToolInvocation\n",
    "# import json\n",
    "# from langchain_core.messages import FunctionMessage\n",
    "\n",
    "\n",
    "# # Define the function that determines whether to continue or not\n",
    "# def should_continue(state):\n",
    "#     messages = state[\"messages\"]\n",
    "#     last_message = messages[-1]\n",
    "#     # If there is no function call, then we finish\n",
    "#     if \"function_call\" not in last_message.additional_kwargs:\n",
    "#         return \"end\"\n",
    "#     elif last_message.additional_kwargs[\"function_call\"][\"name\"] == \"Response\":\n",
    "#         return \"end\"\n",
    "#     # Otherwise if there is, we continue\n",
    "#     else:\n",
    "#         return \"continue\"\n",
    "\n",
    "\n",
    "# # Define the function that calls the model\n",
    "# def call_model(state):\n",
    "#     messages = state[\"messages\"]\n",
    "#     response = model.invoke(messages)\n",
    "#     # We return a list, because this will get added to the existing list\n",
    "#     return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# # Define the function to execute tools\n",
    "# def call_tool(state):\n",
    "#     messages = state[\"messages\"]\n",
    "#     # Based on the continue condition\n",
    "#     # we know the last message involves a function call\n",
    "#     last_message = messages[-1]\n",
    "#     # We construct an ToolInvocation from the function_call\n",
    "#     action = ToolInvocation(\n",
    "#         tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "#         tool_input=json.loads(\n",
    "#             last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "#     # response = input(prompt=f\"[y/n] continue with: {action}?\")\n",
    "#     # if response == \"n\":\n",
    "#     #     raise ValueError(\"User cancelled\")\n",
    "    \n",
    "#     # We call the tool_executor and get back a response\n",
    "#     response = tool_executor.invoke(action)\n",
    "#     # We use the response to create a FunctionMessage\n",
    "#     function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "#     # We return a list, because this will get added to the existing list\n",
    "#     return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d46e188-8e60-4e58-8dac-44c109428c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.graph import StateGraph, END\n",
    "\n",
    "# # Initialize a new graph\n",
    "# graph = StateGraph(AgentState)\n",
    "\n",
    "# # Define the two \"Nodes\"\" we will cycle between\n",
    "# graph.add_node(\"agent\", call_model)\n",
    "# graph.add_node(\"action\", call_tool)\n",
    "\n",
    "# # Define all our \"Edges\"\n",
    "# # Set the \"Starting Edge\" as \"agent\"\n",
    "# # This means that this node is the first one called\n",
    "# graph.set_entry_point(\"agent\")\n",
    "\n",
    "# # We now add a \"Conditional Edge\"\n",
    "# # Conditinal agents take:\n",
    "# # - A start node\n",
    "# # - A function that determines which node to call next\n",
    "# # - A mapping of the output of the function to the next node to call\n",
    "# graph.add_conditional_edges(\n",
    "#     \"agent\",\n",
    "#     should_continue,\n",
    "#     {\n",
    "#         \"continue\": \"action\",\n",
    "#         # END is a special node marking that the graph should finish.\n",
    "#         \"end\": END,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # We now add a \"Normal Edge\" that should always be called after another\n",
    "# graph.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# # We compile the entire workflow as a runnable\n",
    "# app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd946fa-173b-4130-a4d7-5a369c41b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.messages import HumanMessage\n",
    "\n",
    "# inputs = {\n",
    "#     \"messages\": [HumanMessage(content=\"what is the product of 37 and 54 plus 42?\")]\n",
    "# }\n",
    "# for output in app.stream(inputs):\n",
    "#     # stream() yields dictionaries with output keyed by node name\n",
    "#     for key, value in output.items():\n",
    "#         print(f\"Output from node '{key}':\")\n",
    "#         print(\"---\")\n",
    "#         print(value)\n",
    "#     print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d5f0ae-ac7c-40eb-92bc-70b060d0a782",
   "metadata": {},
   "source": [
    "## AutoGen\n",
    "\n",
    "RAGの導入が可能．\n",
    "\n",
    "examples\n",
    "https://microsoft.github.io/autogen/docs/Examples\n",
    "\n",
    "バグが多い！ 実験的なものか？ LangChainで実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e565a22-353c-4fa1-8576-b74c3551ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"mistralai_mistral-7b-instruct-v0.1\", #これが必要！\n",
    "        #\"model\": \"mistralai_mistral-7b-instruct-v0.2\",\n",
    "        #\"model\": \"Phi2\",\n",
    "        \"api_type\": \"openai\",\n",
    "        \"base_url\": \"http://localhost:1234/v1\",\n",
    "        \"api_key\": \"NULL\"\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    " name=\"assistant\",\n",
    " system_message=\"You are a code specializing in Python.\",\n",
    " llm_config=llm_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3299ca-491c-4afc-ba86-ad8d7d76bdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Print hello world\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "```python\n",
      "print(\"Hello World\")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Hello World\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent\n",
    "\n",
    "gpt_assistant = AssistantAgent(\n",
    "    name=\"assistant\", llm_config=llm_config\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    is_termination_msg=lambda msg: \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=1,\n",
    ")\n",
    "user_proxy.initiate_chat(gpt_assistant, message=\"Print hello world\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908c126-f29e-4af5-9f4d-99008efadcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"web\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.\n",
    "Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\",\n",
    ")\n",
    "\n",
    "# # the assistant receives a message from the user, which contains the task description\n",
    "# user_proxy.initiate_chat(\n",
    "#     assistant,\n",
    "#     message=\"\"\"\n",
    "# Who should read this paper: https://arxiv.org/abs/2308.08155\n",
    "# \"\"\",\n",
    "# )\n",
    "\n",
    "# user_proxy.initiate_chat(\n",
    "#     assistant,\n",
    "#     message=\"\"\"Show me the YTD gain of 10 largest technology companies as of today.\"\"\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf23629-79cf-4ec4-b88e-7761e04e4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent named \"assistant\"\n",
    "# assistant = autogen.AssistantAgent(\n",
    "#     name=\"assistant\",\n",
    "#     llm_config={\n",
    "#         \"cache_seed\": 42,  # seed for caching and reproducibility\n",
    "#         \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "#         \"temperature\": 0,  # temperature for sampling\n",
    "#     },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    "# )\n",
    "# # create a UserProxyAgent instance named \"user_proxy\"\n",
    "# user_proxy = autogen.UserProxyAgent(\n",
    "#     name=\"user_proxy\",\n",
    "#     human_input_mode=\"NEVER\",\n",
    "#     max_consecutive_auto_reply=10,\n",
    "#     is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "#     code_execution_config={\n",
    "#         \"work_dir\": \"coding\",\n",
    "#         \"use_docker\": False,  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "#     },\n",
    "# )\n",
    "# # the assistant receives a message from the user_proxy, which contains the task description\n",
    "# user_proxy.initiate_chat(\n",
    "#     assistant,\n",
    "#     message=\"\"\"What date is today? Compare the year-to-date gain for TESLA. Use yfinance package. \"\"\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea02ba2-f2b4-4d4c-ba13-3d5b444bd612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_proxy = autogen.UserProxyAgent(\n",
    "#     name=\"User_proxy\",\n",
    "#     system_message=\"A human admin.\",\n",
    "#     code_execution_config={\n",
    "#         \"last_n_messages\": 3,\n",
    "#         \"work_dir\": \"groupchat\",\n",
    "#         \"use_docker\": False,\n",
    "#     },  \n",
    "# )\n",
    "# coder = autogen.AssistantAgent(\n",
    "#     name=\"Coder\",  # the default assistant agent is capable of solving problems with code\n",
    "#     llm_config=llm_config,\n",
    "# )\n",
    "# critic = autogen.AssistantAgent(\n",
    "#     name=\"Critic\",\n",
    "#     system_message=\"\"\"Critic. You are a helpful assistant highly skilled in evaluating the quality of a given visualization code by providing a score from 1 (bad) - 10 (good) while providing clear rationale. YOU MUST CONSIDER VISUALIZATION BEST PRACTICES for each evaluation. Specifically, you can carefully evaluate the code across the following dimensions\n",
    "# - bugs (bugs):  are there bugs, logic errors, syntax error or typos? Are there any reasons why the code may fail to compile? How should it be fixed? If ANY bug exists, the bug score MUST be less than 5.\n",
    "# - Data transformation (transformation): Is the data transformed appropriately for the visualization type? E.g., is the dataset appropriated filtered, aggregated, or grouped  if needed? If a date field is used, is the date field first converted to a date object etc?\n",
    "# - Goal compliance (compliance): how well the code meets the specified visualization goals?\n",
    "# - Visualization type (type): CONSIDERING BEST PRACTICES, is the visualization type appropriate for the data and intent? Is there a visualization type that would be more effective in conveying insights? If a different visualization type is more appropriate, the score MUST BE LESS THAN 5.\n",
    "# - Data encoding (encoding): Is the data encoded appropriately for the visualization type?\n",
    "# - aesthetics (aesthetics): Are the aesthetics of the visualization appropriate for the visualization type and the data?\n",
    "\n",
    "# YOU MUST PROVIDE A SCORE for each of the above dimensions.\n",
    "# {bugs: 0, transformation: 0, compliance: 0, type: 0, encoding: 0, aesthetics: 0}\n",
    "# Do not suggest code.\n",
    "# Finally, based on the critique above, suggest a concrete list of actions that the coder should take to improve the code.\n",
    "# \"\"\",\n",
    "#     llm_config=llm_config,\n",
    "# )\n",
    "\n",
    "# groupchat = autogen.GroupChat(agents=[user_proxy, coder, critic], messages=[], max_round=20)\n",
    "# manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "# user_proxy.initiate_chat(\n",
    "#     manager,\n",
    "#     message=\"download data from https://raw.githubusercontent.com/uwdata/draco/master/data/cars.csv and plot a visualization that tells us about the relationship between weight and horsepower. Save the plot to a file. Print the fields in a dataset before visualizing it.\",\n",
    "# )\n",
    "# # type exit to terminate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85ebbe-3fec-4946-89cb-cc9af6218e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_proxy = autogen.UserProxyAgent(\n",
    "#     name=\"User_proxy\",\n",
    "#     system_message=\"A human admin.\",\n",
    "#     code_execution_config={\n",
    "#         \"last_n_messages\": 2,\n",
    "#         \"work_dir\": \"groupchat\",\n",
    "#         \"use_docker\": False,\n",
    "#     },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "#     human_input_mode=\"TERMINATE\",\n",
    "# )\n",
    "# coder = autogen.AssistantAgent(\n",
    "#     name=\"Coder\",\n",
    "#     llm_config=llm_config,\n",
    "# )\n",
    "# pm = autogen.AssistantAgent(\n",
    "#     name=\"Product_manager\",\n",
    "#     system_message=\"Creative in software product ideas. Always say something even if you have no idea!\",\n",
    "#     llm_config=llm_config,\n",
    "# )\n",
    "# groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[\"I have no idea.\"], max_round=12)\n",
    "# manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "# user_proxy.initiate_chat(\n",
    "#     manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"\n",
    "# )\n",
    "# # type exit to terminate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bf188-bc20-486c-ab8b-e231663a66cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "# from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "\n",
    "# assistant = RetrieveAssistantAgent(\n",
    "#     name=\"assistant\",\n",
    "#     code_execution_config={\"use_docker\": False},  #　\"use_docker\": Falseを忘れずに！\n",
    "#     system_message=\"You are a helpful assistant.\",\n",
    "#     llm_config=llm_config,\n",
    "# )\n",
    "\n",
    "# ragproxyagent = RetrieveUserProxyAgent(\n",
    "#     name=\"ragproxyagent\",\n",
    "#     code_execution_config={\"use_docker\": False},\n",
    "#     retrieve_config={\n",
    "#         \"task\": \"qa\",\n",
    "#         \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
    "#         #\"docs_path\": \"https://scmopt.github.io/manual/07optseq.html\"\n",
    "#     },\n",
    "# )\n",
    "# ragproxyagent.human_input_mode = \"ALWAYS\"\n",
    "# assistant.reset()\n",
    "# ragproxyagent.initiate_chat(assistant, problem=\"What is autogen?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e227d17-fae3-4162-89ad-0415c40c7b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_proxy = autogen.UserProxyAgent(\n",
    "#  name=\"user_proxy\",\n",
    "#  human_input_mode=\"NEVER\",\n",
    "#  max_consecutive_auto_reply=10,\n",
    "#  is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "#  code_execution_config={\"work_dir\": \"web\", \"use_docker\": False},\n",
    "#  llm_config=llm_config,\n",
    "#  system_message=\"\"\"Replay TERMINATE if the task has been solved at full satisfaction. Otherwise, replay CONTINUE, \n",
    "#  or the reason why the task is not solved yet.\"\"\"\n",
    "# )\n",
    "\n",
    "# task = \"\"\"Write a python method to print numbers 50 to 100\"\"\"\n",
    "\n",
    "# user_proxy.initiate_chat(assistant, message=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533f443c-6661-4663-a8c7-3a9d4cd36c9b",
   "metadata": {},
   "source": [
    "## OpenAIを使用するときのガイドライン"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f188f42c-e954-4bb3-92d1-51f0c92e05b9",
   "metadata": {},
   "source": [
    "### クライアントと関数の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281de7e-a32b-45b5-ab79-ba7d0f9b54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples from https://learn.deeplearning.ai/chatgpt-prompt-eng/\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "def get_completion(prompt, model=\"local-model\", temperature=0):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0 \n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_completion_from_messages(messages, model=\"local-model\", temperature=0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288cdcf0-d63a-4556-a732-53ecbf61bc29",
   "metadata": {},
   "source": [
    "### テキストの要約\n",
    "\n",
    "入力した文章がどの部分かを「明確化」するために区切り文字 (delimiters) を用いる．\n",
    "\n",
    "- 区切り文字の例： ```, \"\"\", < >, `<tag> </tag>`, `:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bae65a-b8b5-4787-8a49-ee9821ac0abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルに望ましい動作を具体的に指示することは重要です。これにより、不適切な回答を受け取る可能性が減り、関連性の高い出力が得られます。\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "モデルに望む動作を明確かつ具体的な指示で表現することが重要です。\\\n",
    "これにより、モデルは望ましい出力に向かって誘導され、不適切な回答を受け取る可能性が減ります。\\\n",
    "明確なプロンプトを書くことと短いプロンプトを書くことを混同しないでください。\\\n",
    "多くの場合、長いプロンプトの方がモデルにとってより明確で、より詳細で関連性の高い出力につながることがあります。\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "<tag> </tag>で囲まれた以下の文章を，日本語の一文に要約してください． \n",
    "<tag>{text}</tag>\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58520c74-59d4-4a65-a2ac-fb57ddb37798",
   "metadata": {},
   "source": [
    "### JSONで出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45fc46-0839-4610-b749-4b903a49c7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"book1\": {\n",
      "    \"book_id\": \"001\",\n",
      "    \"title\": \"幻想の世界\",\n",
      "    \"author\": \"星野光\",\n",
      "    \"genre\": \"ファンタジー\"\n",
      "  },\n",
      "  \"book2\": {\n",
      "    \"book_id\": \"002\",\n",
      "    \"title\": \"未来の扉\",\n",
      "    \"author\": \"水野良樹\",\n",
      "    \"genre\": \"サイエンス・フィクション\"\n",
      "  },\n",
      "  \"book3\": {\n",
      "    \"book_id\": \"003\",\n",
      "    \"title\": \"闇夜の探偵\",\n",
      "    \"author\": \"黒崎竜也\",\n",
      "    \"genre\": \"ミステリー\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "以下のような形式で、3つの架空の本のタイトル、著者、ジャンルをJSON形式で提供してください。\\ \n",
    "キーは、book_id、title、author、genreです。\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e4939-66c4-4770-aec5-3e93a963c6c9",
   "metadata": {},
   "source": [
    "### Step で出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a296f18-b7cd-4256-9d97-005b3e410ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text 1:\n",
      "\n",
      "手順のシーケンスが含まれている場合は、ステップごとの説明とともに簡潔に要約し、書き直します。\n",
      "\n",
      "手順のシーケンスが含まれてない場合は、「手順が提供されてまいせん。」とだけ書いてください。\n"
     ]
    }
   ],
   "source": [
    "text_1 = f\"\"\"\n",
    "お茶を入れるのは簡単です！\\ \n",
    "まず、お湯を沸かす必要があります。\n",
    "それが起こっている間に、カップを取り出して、ティーバッグを入れます。\\\n",
    "お湯が十分に熱くなったら、ティーバッグの上に注ぎます。\n",
    "お茶が浸るようにしばらく置いてから、数分後にティーバッグを取り出します。 \\\n",
    "好みで、砂糖やミルクを加えることができます。\n",
    "以上です！美味しいお茶ができました。\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "<tag> </tag>で区切られたテキストが提供されます。手順のシーケンスが含まれている場合は、次の形式で手順を簡潔に要約して書き直します。\n",
    "\n",
    "ステップ1 - … \n",
    "\n",
    "ステップ2 - … \n",
    "\n",
    "… \n",
    "\n",
    "ステップN - …\n",
    "\n",
    "テキストに手順のシーケンスが含まれていない場合は、「手順が提供されていません。」とだけ書いてください。\n",
    "\n",
    "<tag>{text_1}</tag>\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 1:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b6789-6861-4d6f-9449-e550c8d4bffb",
   "metadata": {},
   "source": [
    "### 例を示す\n",
    "\n",
    "いくつかの解答例を示したあとで，質問をする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ee057-c763-4983-beee-180a51ebb120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "レジリエンスとは、自業自得に陥りがちな生活態を意味します。\n",
      "\n",
      "### Additional Information:\n",
      "\n",
      "このゲームは、子孫から祖父母まで、すべての世代のためのゲームです。\n",
      "\n",
      "### Conclusion:\n",
      "\n",
      "忍耐は、最も美しいスタイルの答えです。\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "あなたの仕事は、一貫したスタイルで答えることです。\n",
    "\n",
    "<child>: 忍耐について教えてください。\n",
    "\n",
    "<grandparent>: 最も深い谷を刻む川は、謙虚な泉から流れ出ています。最も壮大な交響曲は、単一の音符から生まれます。最も複雑なタペストリーは、孤独な糸から始まります。\n",
    "\n",
    "<child>: レジリエンスについて教えてください。\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab401c4b-3f82-4037-8671-2270c1183dc8",
   "metadata": {},
   "source": [
    "### 手順を示す\n",
    "\n",
    "複雑なプロンプトの場合には，手順を示すことによって，マイルストーンを示す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69253700-e7db-4e5a-aa9c-b431acf9e30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for prompt 1:\n",
      "\n",
      "{ \"french_summary\": \"Un beau village où Jack et Jill montent chercher de l'eau dans un puits. Ils chantent en montant, mais malheureusement Jack heurte contre un caillou et se casse légèrement. Ils continuent à explorer avec joie.\" }\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "美しい村で、兄妹のジャックとジルは、丘の上の井戸から水を汲みに出かけました。\\\n",
    "彼らは歌いながら登っていたが、不幸にもジャックは石につまずいて丘を転げ落ち、ジルもついていった。\\\n",
    "少し傷ついたが、2人は慰め合う家に帰りました。\\　\n",
    "不幸にもかかわらず、彼らの冒険心は失われず、喜びを持って探検を続けました。\n",
    "\"\"\"\n",
    "\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "以下の手順を実行してください。 \n",
    "\n",
    "1 - 以下の<tag> </tag>で囲まれたテキストを，日本語の一文に簡潔に要約します。 \n",
    "2 - 要約をフランス語に翻訳します。 \n",
    "3 - フランス語の要約に含まれる名前をリストします。 \n",
    "4 - 次のキーを含むJSONオブジェクトを出力します： \"french_summary\", \"num_names\"。\n",
    "\n",
    "以下にテキストを記載します。\n",
    "<tag>{text}</tag>\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(\"Completion for prompt 1:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2555f-3543-4469-a641-9f8fbca84006",
   "metadata": {},
   "source": [
    "### 出力フォーマットを指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cebdbd-564d-4d42-aaef-8d1c2ec9519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completion for prompt 2:\n",
      "\n",
      "Summary: <summary translation>\n",
      "Translation: <summary translation>\n",
      "Names: <list of names in summary>\n",
      "Output JSON: { \"french_summary\": \"Un beau village où Jack et Jill vont chercher de l'eau dans un puits à flanc de colline. Ils chantent en montant mais malheureusement Jack a glissé et a chuté. Un peu blessé, ils continuent leur aventure. \", \"num_names\": 4 }\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = f\"\"\"\n",
    "Your task is to perform the following actions: \n",
    "1 - Summarize the following text delimited by \n",
    "  <> with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the \n",
    "  following keys: french_summary, num_names.\n",
    "\n",
    "Use the following format:\n",
    "Text: <text to summarize>\n",
    "Summary: <summary>\n",
    "Translation: <summary translation>\n",
    "Names: <list of names in summary>\n",
    "Output JSON: <json with summary and num_names>\n",
    "\n",
    "Text: <{text}>\n",
    "\"\"\"\n",
    "response = get_completion(prompt_2)\n",
    "print(\"\\nCompletion for prompt 2:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb02e6-697f-4e10-a58d-2ae18fde0411",
   "metadata": {},
   "source": [
    "## 逐次改善"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0d8e8-22cb-4227-b888-f4c5441dfbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "製品の説明をマー ケティングチームが作成した技術仕様書をもとに、製品概要を日本語で記述してください。\n",
      "\n",
      "### Additional Information:\n",
      "\n",
      "- ご注意: この仕様書はあくまでの一般的な情報です。詳細については、お問合せください。\n",
      "- ご注意: この仕様書はあくまでの一般的な情報です。詳細については、お問合せください。\n",
      "- ご注意: この仕様書はあくまま の一般な情報です。詳細については、お問合せください。\n"
     ]
    }
   ],
   "source": [
    "fact_sheet_chair = \"\"\"\n",
    "商品概要\n",
    "\n",
    "ファイリングキャビネット、デスク、書棚、会議テーブルなど、美しいミッドセンチュリーインスパイアのオフィス家具の一部です。\n",
    "シェルの色とベースの仕上げにはいくつかのオプションがあります。\n",
    "プラスチックの背面とフロントのアップホルスタリー（SWC-100）または10種類の生地と6種類の革のフルアップホルスタリー（SWC-110）が利用可能です。\n",
    "ベースの仕上げオプションは、ステンレススチール、マットブラック、グロスホワイト、またはクロムです。\n",
    "アームレスト付きまたはアームレストなしで利用可能です。\n",
    "家庭用またはビジネス用の設定に適しています。\n",
    "契約使用に適格です。\n",
    "\n",
    "CONSTRUCTION\n",
    "- 5輪のプラスチックコーティングされたアルミベース。\n",
    "- ニューマチックチェアアジャストで簡単に上下動作します。\n",
    "\n",
    "DIMENSIONS\n",
    "- 幅53cm | 20.87インチ\n",
    "- 奥行き51cm | 20.08インチ\n",
    "- 高さ80cm | 31.50インチ\n",
    "- シート高44cm | 17.32インチ\n",
    "- シート奥行き41cm | 16.14インチ\n",
    "\n",
    "OPTIONS\n",
    "- ソフトまたはハードフロアキャスターオプション。\n",
    "- シートフォーム密度の2つの選択肢：中程度（1.8ポンド/立方フィート）または高密度（2.8ポンド/立方フィート）\n",
    "- アームレスまたは8ポジションPUアームレスト\n",
    "\n",
    "MATERIALS SHELL BASE GLIDER\n",
    "- 改良ナイロンPA6 / PA66コーティングを施した鋳造アルミニウム。\n",
    "- シェル厚：10 mm。 SEAT\n",
    "- HD36フォーム\n",
    "\n",
    "COUNTRY OF ORIGIN\n",
    "-イタリア\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "あなたのタスクは、技術仕様書に基づいた製品の小売ウェブサイトの説明をマーケティングチームが作成することです。\\\n",
    "以下の<tag> </tag>で囲まれた技術仕様書の情報をもとに，製品概要を日本語で書いてください，\n",
    "\n",
    "技術仕様書: <tag>{fact_sheet_chair}</tag>\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80671a1a-aac5-473f-b2e4-b0e8b2be4193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "家具の説明を書くには、技術仕様書に記載されている情報に基づいて行ってください。\n",
      "\n",
      "説明は小売業向けであり、技術的であり、製品の構築材料に焦点を当てる必要があります。\n",
      "\n",
      "説明の後に、製品の寸法を示すテーブルを追加してください。テーブルには2つの列を含めます。\n",
      "最初の列には寸法の名前、2番目の列にはインチ単位での測定値を含めてください。\n",
      "\n",
      "全てをウェブサイトで使用できるHTML形式にしてください。説明を<div>要素に配置してください。\n",
      "\n",
      "技術仕様書: <tag> \n",
      "商品概要\n",
      "\n",
      "ファイリングキャビネット、デスク、書棚、会議テーブルなど、美しいミッドセンチュリーインスパイアのオフィス家具の一部です。シェルの色とベースの仕上げにはいくつかのオプションがあります。プラスチックの背面とフロントのアップホルスタリー（SWC-100）または10種類の生地と6種類の革のフルアップホルスタリー (SWC-110) が利用可能です。 アームレススト付きまたはアームレスストなしで利用可能です。家庭用またはビジネス用の設定に適格です。契約使用に適格です。\n",
      "\n",
      "CONSTRUCTION\n",
      "- 5輪のプラスチックコーティングされたアルミベース。\n",
      "- ニューマチックチェアアジャストで簡単に上下動作します。\n",
      "\n",
      "DIMENSIONS\n",
      "- 幅53cm | 20.87インチ\n",
      "- 奥行き51cm | 20.08インチ\n",
      "- 高さ80cm | 31.50インチ\n",
      "- シート高44cm | 17.32インチ\n",
      "- シート奥行き41cm | 16.14インチ\n",
      "\n",
      "OPTIONS\n",
      "- ソフトまたはハードフロアキャスターオプション。\n",
      "- シートフォーム密度の2つの選択肢：中程度 (1.8ポンド/立方フィート) または高密度 (2.8ポンド/立方フィート) \n",
      "- アームレスまたは8ポジションPUアームレスト\n",
      "\n",
      "MATERIALS SHELL BASE GLIDER\n",
      "- 改良ナイロンPA6 / PA66コーティングを施した鋳造アルミニウム。\n",
      "- シェル厚：10 mm. SEAT\n",
      "- HD36フォーム\n",
      "\n",
      "COUNTRY OF ORIGIN\n",
      "-イタリア\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "私たちの仕事は、マーケティングチームが技術仕様書に基づいて小売ウェブサイトの製品の説明を作成する手助けをすることです。\n",
    "\n",
    "<tag> </tag>で囲まれたで囲まれた技術仕様書に記載された情報に基づいて製品説明を書いてください。\n",
    "\n",
    "説明は家具小売業者向けであり、技術的であり、製品の構築材料に焦点を当てる必要があります。\n",
    "\n",
    "説明の最後に、技術仕様書に含まれる7文字の製品IDをすべて含めてください。\n",
    "\n",
    "説明の後に、製品の寸法を示すテーブルを追加してください。テーブルには2つの列を含めます。\n",
    "最初の列には寸法の名前を、2番目の列にはインチ単位での測定値を含めてください。\n",
    "\n",
    "テーブルに「製品寸法」というタイトルを付けてください。\n",
    "\n",
    "すべてをウェブサイトで使用できるHTML形式にしてください。説明を<div>要素に配置してください。\n",
    "\n",
    "技術仕様書： <tag> {fact_sheet_chair}</tag>\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "# display(HTML(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb9685-e8f4-43f9-bfd0-0982f6302072",
   "metadata": {},
   "source": [
    "## メッセージの履歴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa06cbd-4026-4ce9-b938-b9340867fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are friendly chatbot.'},    \n",
    "{'role':'user', 'content':'Yes,  can you remind me, What is my name?'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df5efc-21c2-4796-a431-1ee812712241",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are friendly chatbot.'},\n",
    "{'role':'user', 'content':'Hi, my name is Isa'},\n",
    "{'role':'assistant', 'content': \"Hi Isa! It's nice to meet you. \\\n",
    "Is there anything I can help you with today?\"},\n",
    "{'role':'user', 'content':'Yes, you can remind me, What is my name?'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fdf50-7028-4afe-ba28-925bcfc6e595",
   "metadata": {},
   "source": [
    "## チャットボットの例 (gradio) \n",
    "\n",
    "Streamlitの方が柔軟！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd355ff-b4a0-4225-9747-633212d327d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "    \n",
    "def predict(message, history):\n",
    "    history_openai_format = []\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human })\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"local-model\", # this field is currently unused\n",
    "        messages=history_openai_format,\n",
    "        temperature=0.\n",
    "    )\n",
    "    yield response.choices[0].message.content\n",
    "\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca456b2-0fdc-4810-941b-6f6bd03a7a6e",
   "metadata": {},
   "source": [
    "## 要約"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ab573-aa2c-485f-8fef-d2d9ab972437",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_review = \"\"\"\n",
    "娘の誕生日にこのパンダのぬいぐるみを買いました。\\\n",
    "彼女は大好きで、どこへでも持って行きます。柔らかくてとてもかわいいです。顔も優しい表情です。\\\n",
    "ただ、支払った金額に対して少し小さめです。\\\n",
    "同じ値段でより大きな選択肢があるかもしれません。\\\n",
    "予想より1日早く届いたので、渡す前に自分で遊ぶことができました。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d002bfe-b867-40a2-b006-b3c8b8f82807",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "あなたのタスクは、ECサイトからの製品レビューの短い要約を生成することです。\n",
    "\n",
    "以下の<tag> </tag> で囲まれたレビューの文章を簡潔に要約してください。\n",
    "\n",
    "レビュー: <tag>{prod_review}</tag>\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09294faa-f868-479b-8ea6-df047d3efb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "あなたの仕事は、商品のレビューから配送部門へのフィードバックを提供するために、ECサイトの製品レビューの短い要約を生成することです。|\n",
    "以下の<tag> </tag> で囲まれたレビューを商品の配送と納品に関する側面に焦点を当てて，簡潔に要約してください．\n",
    "\n",
    "レビュー: <tag>{prod_review}</tag>\n",
    "\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9bf79-03d1-488d-9a62-28d2f68ac41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "あなたの仕事は、製品の価格を決定する責任がある価格部門にフィードバックを提供するために、ECサイトの製品レビューの短い要約を生成することです。\n",
    "以下の<tag> </tag> で囲まれたレビューを、価格と価値に関連する側面に焦点を当てて簡潔に要約してください．\n",
    "レビュー: <tag>{prod_review}</tag>\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d45a02-def0-4fe0-aee2-c255fee11a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "review_1 = \"\"\"\n",
    "私の寝室用に素敵なランプが必要で、このランプは追加の収納スペースがあり、\n",
    "価格も高すぎないと思いました。2日で到着しました。輸送中にランプのひもが切れましたが、\n",
    "会社は喜んで新しいものを送ってくれました。数日で届きました。\n",
    "組み立ても簡単でした。しかし、欠けている部品があり、サポートに連絡したところ、\n",
    "すぐに欠けている部品を手配してくれました！\n",
    "顧客と製品を大切にする素晴らしい会社だと思います。\n",
    "\"\"\"\n",
    "\n",
    "review_2 = \"\"\"\n",
    "私の歯科衛生士が電動歯ブラシを勧めたので、これを手に入れました。\n",
    "バッテリーの持ちがかなり印象的です。\n",
    "初期充電後、1週間はバッテリーを調整するために充電器を接続しましたが、その後、充電器を抜き、同じ充電で3週間使用しています。\n",
    "しかし、歯ブラシの頭は小さすぎます。この歯ブラシより大きな赤ちゃん用歯ブラシを見たことがあります。\n",
    "歯の間をより良く掃除するために、頭が異なる長さの毛で大きくなると良いと思います。\n",
    "全体的に、50ドルくらいで手に入れられれば、お得です。\n",
    "メーカーの交換用ヘッドはかなり高価ですが、より手頃な価格の汎用品もあります。\n",
    "この歯ブラシは、毎日歯医者に行ったような気分にさせます。歯がキラキラと綺麗に感じます！\n",
    "\"\"\"\n",
    "\n",
    "review_3 = \"\"\"\n",
    "11月には、17ピースのセットが季節限定セールで約49ドルでまだ販売されていましたが、半額程度でした。\n",
    "しかし、12月の第二週頃、価格がどれも約70ドルから89ドルに上昇しました（これを価格操作と呼びましょう）。\n",
    "同じシステムの11ピースセットも、29ドルという以前のセール価格から約10ドル値上がりしました。\n",
    "見た目はまあまあですが、ベースを見ると、ブレードがロックされる部分が数年前の以前のモデルほど良く見えませんが、\n",
    "私はとても丁寧に使うつもりです（例えば、豆、氷、米などの非常に固い食材をまずブレンダーで粉砕し、\n",
    "それをブレンダーで欲しいサイズにパルヴァしてから、細かい粉にするために泡立てブレードに切り替え\n",
    "、スムージーを作る際にはクロスカットブレードを先に使用し、フラットブレードを使用してより細かく/果肉の少ない場合）。\n",
    "スムージーを作る際の特別なコツは、フルーツや野菜を細かく切って凍らせることです（ほうれん草を使用する場合は、\n",
    "ほうれん草を軽く煮込んで柔らかくし、使用する準備ができるまで冷凍し、ソルベを作る場合は、\n",
    "小〜中サイズのフードプロセッサーを使用してください）。それにより、スムージーを作る際に氷をあまり多く加えなくても済みます。\n",
    "約1年後、モーターが奇妙な音を立て始めました。カスタマーサービスに連絡しましたが、すでに保証期間が切れていたため、別のものを買わなければなりませんでした。\n",
    "ご参考までに：この種の製品全体の品質が低下しているので、ブランド認知度と消費者の忠誠心に頼って売上を維持しています。約2日で届きました。\n",
    "\"\"\" \n",
    "\n",
    "reviews = [review_1,review_2, review_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50854f7-90e8-47a6-9d73-af50c37e9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):\n",
    "    prompt = f\"\"\"\n",
    "    あなたの仕事は、商品のレビューから配送部門へのフィードバックを提供するために、ECサイトの製品レビューの短い要約を生成することです。|\n",
    "    以下の<tag> </tag> で囲まれたレビューを，なるべく簡潔に要約してください．\n",
    "    \n",
    "    レビュー: <tag>{reviews[i]}</tag>\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    response = get_completion(prompt)\n",
    "    print(i, response, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892cb04f-18f6-4628-9fda-034bc61a4f96",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a2655-2efd-44ec-82a9-fbd01123c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following product review, \n",
    "which is delimited with triple backticks?\n",
    "\n",
    "Review text: '''{prod_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d48d2-ab33-4ca8-8d9d-fa6cd4552b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):\n",
    "    prompt = f\"\"\"\n",
    "    What is the sentiment of the following product review, \n",
    "    which is delimited with triple backticks?\n",
    "    \n",
    "    Review text: '''{reviews[i]}'''\n",
    "    \"\"\" \n",
    "    response = get_completion(prompt)\n",
    "    print(i, response, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36adb6bf-eda1-4060-a200-60989bdf8ef4",
   "metadata": {},
   "source": [
    "## 拡張"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defff240-eb64-4ca3-878d-22d467b3ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = review_2\n",
    "sentiment = \"positive\"\n",
    "#sentiment = \"negative\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "あなたは顧客サービスのAIアシスタントです。\n",
    "\n",
    "あなたの仕事は、価値ある顧客にメール返信を日本語で送ることです。\n",
    "顧客のメールは， 以下の<tag> </tag> で囲まれた文で与えられます．\n",
    "\n",
    "```で囲まれた文でユーザーの感情（sentiment）が与えられます．\n",
    "\n",
    "感情が positive または neutral の場合、彼らのレビューに感謝します。\n",
    "\n",
    "感情が nagative な場合、謝罪し、顧客サービスに連絡することを提案します。\n",
    "\n",
    "簡潔でプロフェッショナルなトーンで書いてください。\n",
    "メールの署名は `AIカスタマーエージェント` とします。\n",
    "\n",
    "顧客のメール: <tag>{review}</tag>\n",
    "sentiment: ```{sentiment}```\n",
    "\"\"\" \n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b01fadf-9e4c-468d-9591-0718fb5272d3",
   "metadata": {},
   "source": [
    "## 翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd98530-179f-46fd-a57d-84888ae6e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate the following English text to Spanish: \\ \n",
    "```Hi, I would like to order a blender```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7111262-00a3-40ab-be8a-cf6fbf97f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Tell me which language this is: \n",
    "```Combien coûte le lampadaire?```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5084dee7-4479-4e21-a8ad-03474328bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate the following  text to French and Spanish\n",
    "and English pirate: \\\n",
    "```I want to order a basketball```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cadf06-6b1b-48da-ae35-93d1d6e593c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Translate the following text to Spanish in both the \\\n",
    "formal and informal forms: \n",
    "'Would you like to order a pillow?'\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74940e07-3c30-471a-b292-97fd5bd089f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_messages = [\n",
    "  \"La performance du système est plus lente que d'habitude.\",  # System performance is slower than normal         \n",
    "  \"Mi monitor tiene píxeles que no se iluminan.\",              # My monitor has pixels that are not lighting\n",
    "  \"Il mio mouse non funziona\",                                 # My mouse is not working\n",
    "  \"Mój klawisz Ctrl jest zepsuty\",                             # My keyboard has a broken control key\n",
    "  \"我的屏幕在闪烁\"                                               # My screen is flashing\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1860e373-9186-4228-9f5a-f7bcf9e57a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for issue in user_messages:\n",
    "    prompt = f\"Tell me what language this is: ```{issue}```\"\n",
    "    lang = get_completion(prompt)\n",
    "    print(f\"Original message ({lang}): {issue}\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Translate the following  text to English \\\n",
    "    and Japanese: ```{issue}```\n",
    "    \"\"\"\n",
    "    response = get_completion(prompt)\n",
    "    print(response, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0aff61-7966-4a12-8340-ad65a25a2ada",
   "metadata": {},
   "source": [
    "## 分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9137bb2-a87c-46ab-9dc8-6e2c820c7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "\n",
    "system_message = f\"\"\"\n",
    "客サービスの問い合わせが提供されます。\n",
    "顧客サービスの問い合わせは、{delimiter}文字で区切られます。\n",
    "各問い合わせを主要カテゴリと副カテゴリに分類します。\n",
    "出力は、主要カテゴリと副カテゴリのキーを持つJSON形式で提供します。\n",
    "\n",
    "主要カテゴリ： 請求、技術サポート、アカウント管理、または一般的な問い合わせ。\n",
    "\n",
    "請求の副カテゴリ：\n",
    "- 解約またはアップグレード\n",
    "- 支払い方法の追加\n",
    "- 請求に関する説明\n",
    "- 請求の異議申し立て\n",
    "\n",
    "技術サポートの副カテゴリ：\n",
    "- 一般的なトラブルシューティング\n",
    "- デバイスの互換性\n",
    "- ソフトウェアのアップデート\n",
    "\n",
    "アカウント管理の副カテゴリ：\n",
    "- パスワードリセット\n",
    "- 個人情報の更新\n",
    "- アカウントの閉鎖\n",
    "- アカウントのセキュリティ\n",
    "\n",
    "一般的な問い合わせの副カテゴリ：\n",
    "- 製品情報\n",
    "- 価格設定\n",
    "- フィードバック\n",
    "- 人間と話す\n",
    "\"\"\"\n",
    "\n",
    "user_message = f\"\"\"\\\n",
    "私のプロフィールとすべてのユーザーデータを削除してほしいです。\"\"\"\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faefe14-8c23-45ec-aba0-ccd83117a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = f\"\"\"\\\n",
    "フラットスクリーンテレビについてもっと教えてください。\"\"\"\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6361c-240e-403d-b05a-73eaf7e2200a",
   "metadata": {},
   "source": [
    "## 製品の分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b7242-712d-4040-9155-066e983147ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "You will be provided with customer service queries. \\\n",
    "The customer service query will be delimited with \\\n",
    "{delimiter} characters.\n",
    "Output a python list of objects, where each object has \\\n",
    "the following format:\n",
    "    'category': <one of Computers and Laptops, \\\n",
    "    Smartphones and Accessories, \\\n",
    "    Televisions and Home Theater Systems, \\\n",
    "    Gaming Consoles and Accessories, \n",
    "    Audio Equipment, Cameras and Camcorders>,\n",
    "OR\n",
    "    'products': <a list of products that must \\\n",
    "    be found in the allowed products below>\n",
    "\n",
    "Where the categories and products must be found in \\\n",
    "the customer service query.\n",
    "If a product is mentioned, it must be associated with \\\n",
    "the correct category in the allowed products list below.\n",
    "If no products or categories are found, output an \\\n",
    "empty list.\n",
    "\n",
    "Allowed products: \n",
    "\n",
    "Computers and Laptops category:\n",
    "TechPro Ultrabook\n",
    "BlueWave Gaming Laptop\n",
    "PowerLite Convertible\n",
    "TechPro Desktop\n",
    "BlueWave Chromebook\n",
    "\n",
    "Smartphones and Accessories category:\n",
    "SmartX ProPhone\n",
    "MobiTech PowerCase\n",
    "SmartX MiniPhone\n",
    "MobiTech Wireless Charger\n",
    "SmartX EarBuds\n",
    "\n",
    "Televisions and Home Theater Systems category:\n",
    "CineView 4K TV\n",
    "SoundMax Home Theater\n",
    "CineView 8K TV\n",
    "SoundMax Soundbar\n",
    "CineView OLED TV\n",
    "\n",
    "Gaming Consoles and Accessories category:\n",
    "GameSphere X\n",
    "ProGamer Controller\n",
    "GameSphere Y\n",
    "ProGamer Racing Wheel\n",
    "GameSphere VR Headset\n",
    "\n",
    "Audio Equipment category:\n",
    "AudioPhonic Noise-Canceling Headphones\n",
    "WaveSound Bluetooth Speaker\n",
    "AudioPhonic True Wireless Earbuds\n",
    "WaveSound Soundbar\n",
    "AudioPhonic Turntable\n",
    "\n",
    "Cameras and Camcorders category:\n",
    "FotoSnap DSLR Camera\n",
    "ActionCam 4K\n",
    "FotoSnap Mirrorless Camera\n",
    "ZoomMaster Camcorder\n",
    "FotoSnap Instant Camera\n",
    "\n",
    "Only output the list of objects, with nothing else.\n",
    "\"\"\"\n",
    "\n",
    "user_message_1 = f\"\"\"\n",
    " tell me about the smartx pro phone and \\\n",
    " the fotosnap camera, the dslr one. \\\n",
    " Also tell me about your tvs \"\"\"\n",
    "\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message_1}{delimiter}\"},  \n",
    "] \n",
    "category_and_product_response_1 = get_completion_from_messages(messages)\n",
    "print(category_and_product_response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4aad00-0b93-4830-9b3e-67f0524be801",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cda02c-2a44-4821-bc95-69d179b419c6",
   "metadata": {},
   "source": [
    "### Chat: API\n",
    "\n",
    "ChatOpenAIクラスでchatオブジェクトを生成する．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba01f08-e3c4-4d7b-ba74-b008fdabff72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object>, async_client=<openai.resources.chat.completions.AsyncCompletions object>, model_name='local-model', temperature=0.0, openai_api_key='not-needed', openai_api_base='http://localhost:1234/v1', openai_proxy='')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI #pip install langchain-openai が必要（OpenAIパッケージは古い）\n",
    "chat = ChatOpenAI(temperature=0.0, base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\", model=\"local-model\")\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7a086a-de0e-4e18-ad1d-2a0ffb88feae",
   "metadata": {},
   "source": [
    "ChatPromptTemplateは，テンプレート生成のためのクラスである．\n",
    "\n",
    "文字列を与えると {} で囲んだ部分を引数としたテンプレートオブジェクトを生成する．\n",
    "\n",
    "生成されたテンプレートオブジェクトに， format_messages() メソッドから引数を与えると，メッセージが生成される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7d61e-7c75-4009-8f97-cb6f488963aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['style', 'text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'))])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e707e-21ec-497c-b0b4-7a20e3665062",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b21fb7-b1ed-4f67-9101-b2300d05748a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\")]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)\n",
    "customer_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb4975-7736-46cf-a940-67bb9fa64006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is the translated text in American English:\\n\\n\"I\\'m extremely frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! To make matters even worse, the warranty doesn\\'t cover the cost of cleaning up my kitchen. I really need your help right now.\"')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat.invoke(customer_messages)\n",
    "customer_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a5e6f-7388-4c40-83b5-2791acd3b6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there me hearty! Ye be seekin' a translation o' this text, be ye? Well, gather 'round and listen close, for I shall endeavor t' translate it into the style o' English Pirate. Here goes nothin':\n",
      "\n",
      "\"Arr, me hearty customer! The warranty be not coverin' yer cleanin' expenses fer yer kitchen, see? It's yer fault, ye be misusin' yer blender by forgettin' t' put the lid on before startin' it. Tough luck, matey! Off we go!\"\n"
     ]
    }
   ],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\"\n",
    "\n",
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\"\n",
    "\n",
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "service_response = chat.invoke(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e9a0ac-196f-4322-a360-85afcd8ce5f7",
   "metadata": {},
   "source": [
    "### Output Parser\n",
    "\n",
    "プロンプトに出力を指定することもできるが，Parserを用いた方法を紹介する．\n",
    "\n",
    "まずは普通にJSONで出力するように指定する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd634f-84c4-4d4a-ba33-e0c810f3a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bcab05-d48e-41df-9b65-1217ebf68ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'))]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b905ec-03cd-47eb-a3d9-a743ef2ccbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the extracted information in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"gift\": true,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there\"]\n",
      "}\n",
      "```\n",
      "\n",
      "Note that I've assumed that \"my wife's anniversary present\" implies that the item was purchased as a gift, so I set `gift` to `true`. If you'd like to be more conservative and set it to `unknown`, I can do that instead:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"gift\": \"unknown\",\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOpenAI(temperature=0.0, base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\", model=\"local-model\")\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b7a1d-975f-4e18-9b1c-c3c4491557e3",
   "metadata": {},
   "source": [
    "ResponseSchemaでスキーマを定義しておき，スキーマのリストを準備する．\n",
    "\n",
    "StructuredOutputParserクラスのfrom_response_schemas()メソッドにスキーマのリストを渡すことによって，\n",
    "出力パーサーのインスタンス output_parser ができる．\n",
    "\n",
    "output_parserのget_format_instructions()メソッドで，スキーマで定義されたフォーマットにしたがった出力が得られる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ee4aa-03ff-4e2f-8f79-a68e794208ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401f681-50b3-4512-bfd8-321d4c368f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df201f7-b911-407b-9de7-8bdb4c17f200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\\n\\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\\n\\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\\n}\\n```'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aca177-30fb-4c8f-8829-c71590e669a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"gift\": True,\n",
      "    \"delivery_days\": \"2\",\n",
      "    \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                  format_instructions=format_instructions)\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfcc2b5-89b9-4f33-936c-9c08ef97932a",
   "metadata": {},
   "source": [
    "### Chain\n",
    "\n",
    "LLMChainインスタンスを複数まとめて SequentialChain クラスの引数 chains に入れて， プロンプトを順次実行する連鎖プロンプトができる．\n",
    "\n",
    "Review => English Review => Summary => language  => follow up message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3408c1-9fea-4d49-a3b7-3c5697580c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5361b76-1297-4a98-ab41-78d1e895d1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature= 0.0, base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035fbb0-84c1-4b74-b2d0-156cc7513e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product': 'Queen Size Sheet Set',\n",
       " 'text': \"Here are some suggestions for a company name that makes Queen Size Sheet Sets:\\n\\n1. **Royal Slumber**: This name plays off the idea of a queen-sized bed and evokes feelings of luxury and comfort.\\n2. **DreamWeaver Bedding**: This name suggests that the company's sheets will help customers weave their own dreams, which is perfect for a product designed to promote restful sleep.\\n3. **Queenly Comforts**: This name emphasizes the idea of comfort and coziness associated with queen-sized beds.\\n4. **Sleepytime Sheets**: This name has a fun, playful tone and implies that the company's sheets will help customers drift off to dreamland.\\n5. **Regal Rest**: This name conveys a sense of luxury and sophistication, suggesting that the company's sheets are fit for royalty.\\n6. **Bedding Bliss**: This name emphasizes the idea of finding happiness and contentment in one's bed, which is exactly what customers want from their sheet sets.\\n7. **Queen Size Comforts Co.**: This name is straightforward and to the point, emphasizing the size and comfort of the company's products.\\n\\nWhich one do you like best? Or would you like me to suggest more options?\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "chain.invoke(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afdbb6b-809c-4e03-8635-df3d1fb693eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature= 0.0, base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n",
    "\n",
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n",
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n",
    "\n",
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ecfa12-74f5-40f0-a51c-2a341aeff2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': '\\n11月には、17ピースのセットが季節限定セールで約49ドルでまだ販売されていましたが、半額程度でした。\\nしかし、12月の第二週頃、価格がどれも約70ドルから89ドルに上昇しました（これを価格操作と呼びましょう）。\\n同じシステムの11ピースセットも、29ドルという以前のセール価格から約10ドル値上がりしました。\\n見た目はまあまあですが、ベースを見ると、ブレードがロックされる部分が数年前の以前のモデルほど良く見えませんが、\\n私はとても丁寧に使うつもりです（例えば、豆、氷、米などの非常に固い食材をまずブレンダーで粉砕し、\\nそれをブレンダーで欲しいサイズにパルヴァしてから、細かい粉にするために泡立てブレードに切り替え\\n、スムージーを作る際にはクロスカットブレードを先に使用し、フラットブレードを使用してより細かく/果肉の少ない場合）。\\nスムージーを作る際の特別なコツは、フルーツや野菜を細かく切って凍らせることです（ほうれん草を使用する場合は、\\nほうれん草を軽く煮込んで柔らかくし、使用する準備ができるまで冷凍し、ソルベを作る場合は、\\n小〜中サイズのフードプロセッサーを使用してください）。それにより、スムージーを作る際に氷をあまり多く加えなくても済みます。\\n約1年後、モーターが奇妙な音を立て始めました。カスタマーサービスに連絡しましたが、すでに保証期間が切れていたため、別のものを買わなければなりませんでした。\\nご参考までに：この種の製品全体の品質が低下しているので、ブランド認知度と消費者の忠誠心に頼って売上を維持しています。約2日で届きました。\\n',\n",
       " 'English_Review': 'Here is the translation of the review to English:\\n\\n\"In November, a 17-piece set was still on sale for around $49 with a discount of about half off. However, by mid-December, all prices had risen to around $70-$89 (let\\'s call it price manipulation).\\nThe same system\\'s 11-piece set also increased in price from the previous sale price of $29 to around $10 more.\\nAs for appearance, it\\'s just okay, but when you look at the base, the part where the blade locks is not as good-looking as the older model from a few years ago. But I plan to use it very carefully (for example, first crushing hard ingredients like beans, ice, and rice in the blender, then using the paravation blade to get the desired size, and finally switching to the foam blade to make fine powder).\\nThe special trick for making smoothies is to finely chop fruits and vegetables and freeze them beforehand. For example, if you\\'re using spinach, blanch it lightly to soften it, freeze it until ready to use, and use a small-to-medium-sized food processor when making sorbet). This way, you can add less ice when making smoothies.\\nAbout a year later, the motor started making strange noises. I contacted customer service, but since the warranty had already expired, I couldn\\'t get a replacement.\\n\\nNote: Just to mention that this type of product has overall poor quality, so they rely on brand recognition and consumer loyalty to maintain sales. It arrived in about 2 days.\"\\n\\nIt seems like the reviewer was generally satisfied with the product\\'s performance, but had some issues with its durability (the motor started making strange noises after a year) and felt that the price manipulation was unfair. They also provided some tips for using the product effectively.',\n",
       " 'summary': \"Here is a summary of the review in 1 sentence:\\n\\nThe reviewer was generally satisfied with the product's performance, but had issues with its durability (motor noise after a year) and felt that the manufacturer engaged in price manipulation, while also providing helpful tips on how to use it effectively.\",\n",
       " 'followup_message': 'Here is a follow-up response in Japanese:\\n\\n「製品の性能は大体満足だったが、耐久性については問題があった（1年後にはモーター音が出始めた）と述べている。また、価格操作を行ったと非難し、しかし使用方法については役に立つアドバイスを提供していた」という点で、製品の評価は微妙なものとなっているようだ。'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = \"\"\"\n",
    "11月には、17ピースのセットが季節限定セールで約49ドルでまだ販売されていましたが、半額程度でした。\n",
    "しかし、12月の第二週頃、価格がどれも約70ドルから89ドルに上昇しました（これを価格操作と呼びましょう）。\n",
    "同じシステムの11ピースセットも、29ドルという以前のセール価格から約10ドル値上がりしました。\n",
    "見た目はまあまあですが、ベースを見ると、ブレードがロックされる部分が数年前の以前のモデルほど良く見えませんが、\n",
    "私はとても丁寧に使うつもりです（例えば、豆、氷、米などの非常に固い食材をまずブレンダーで粉砕し、\n",
    "それをブレンダーで欲しいサイズにパルヴァしてから、細かい粉にするために泡立てブレードに切り替え\n",
    "、スムージーを作る際にはクロスカットブレードを先に使用し、フラットブレードを使用してより細かく/果肉の少ない場合）。\n",
    "スムージーを作る際の特別なコツは、フルーツや野菜を細かく切って凍らせることです（ほうれん草を使用する場合は、\n",
    "ほうれん草を軽く煮込んで柔らかくし、使用する準備ができるまで冷凍し、ソルベを作る場合は、\n",
    "小〜中サイズのフードプロセッサーを使用してください）。それにより、スムージーを作る際に氷をあまり多く加えなくても済みます。\n",
    "約1年後、モーターが奇妙な音を立て始めました。カスタマーサービスに連絡しましたが、すでに保証期間が切れていたため、別のものを買わなければなりませんでした。\n",
    "ご参考までに：この種の製品全体の品質が低下しているので、ブランド認知度と消費者の忠誠心に頼って売上を維持しています。約2日で届きました。\n",
    "\"\"\" \n",
    "overall_chain.invoke(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c7095-10db-4e1e-8b58-380caa7bbd78",
   "metadata": {},
   "source": [
    "分岐のある連鎖も LLMRouterChain クラスを用いればできる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798b880-1ad8-4198-8563-5d7fe661f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd915402-a09a-42ca-8414-83eef68804ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267fa66-d61d-489e-9801-853c04ba948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf677f4-3a4e-4839-8aef-1bf01fbd9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8833fd1-6040-4c4b-bbfa-8592ae0febbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38394b8-4c81-46e6-bc31-601356bc93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793a81a-7d01-4526-a917-c24163520d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a1841-f7c1-4072-a5dc-684f73572d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2b90b-e57d-4901-bc51-dfe7721933e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91cb187-a36b-4f76-a52a-259f5cf1ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26324cb-f632-4206-8f67-70b6f28e2df4",
   "metadata": {},
   "source": [
    "## SCML LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c99da5-fea5-46b3-8496-7b24aadab2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# prompt = f\"\"\"\n",
    "# Translate the following English text to as many languages as you can: \\ \n",
    "# ```My name is AI assistant. Please tell me your name and affiliation ?```\n",
    "# \"\"\"\n",
    "# response = get_completion(prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b4ce1-0a47-4af6-a97f-664b3ba51f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# 1. French: \n",
    "#    - \"Mon nom est assistant IA. Veuillez me dire votre nom et votre affiliation ?\"\n",
    "\n",
    "# 2. Spanish:\n",
    "#    - \"Mi nombre es asistente de IA. ¿Por favor, podrías decirme tu nombre y afiliación?\"\n",
    "\n",
    "# 3. German:\n",
    "#    - \"Mein Name ist KI-Assistent. Bitte sagen Sie mir Ihren Namen und Ihre Zugehörigkeit?\"\n",
    "\n",
    "# 4. Italian:\n",
    "#    - \"Il mio nome è assistente AI. Per favore, mi dica il suo nome e la sua affiliazione?\"\n",
    "\n",
    "# 5. Portuguese:\n",
    "#    - \"Meu nome é assistente de IA. Por favor, me diga seu nome e afiliação?\"\n",
    "\n",
    "# 6. Russian:\n",
    "#    - \"Меня зовут искусственный интеллект. Пожалуйста, скажите мне ваше имя и аффилиацию?\"\n",
    "\n",
    "# 7. Chinese (Simplified):\n",
    "#    - \"我的名字是人工智能助手。请告诉我您的姓名和所属单位？\"\n",
    "\n",
    "# 8. Japanese:\n",
    "#    - \"私の名前はAIアシスタントです。あなたの名前と所属を教えてください？\"\n",
    "\n",
    "# 9. Arabic:\n",
    "#    - \"اسمي مساعد الذكاء الاصطناعي. من فضلك قل لي اسمك وانتماؤك؟\"\n",
    "\n",
    "# 10. Dutch:\n",
    "#     - \"Mijn naam is AI-assistent. Vertel me alstublieft uw naam en affilliatie?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf995285-d886-4075-a337-b33efb148f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from langchain_openai import ChatOpenAI, OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14740ff9-b4f3-4f60-a070-30bbb56a5ef6",
   "metadata": {},
   "source": [
    "#### 配送計画モデルの諸パラメータを抽出するプロンプトを生成する関数 extract_vrp_info\n",
    "\n",
    "引数：\n",
    "\n",
    "- user_message: ユーザーメッセージ\n",
    "\n",
    "返値：\n",
    "\n",
    "- message: LLMに入れる文字列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c9e51-ea98-463a-8a9c-18ae09acdf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_vrp_info(user_message: str) -> str: \n",
    "    #Output Parser\n",
    "    tw_schema = ResponseSchema(name=\"time windows\", description=\"number of time windows\", type=\"string\")\n",
    "    dimension_schema = ResponseSchema(name=\"dimensions\", description=\"number of dimensions\", type=\"string\")\n",
    "    skill_schema = ResponseSchema(name=\"skills\", description=\"number of skills\", type=\"string\")\n",
    "    break_schema = ResponseSchema(name=\"breaks\", description=\"number of breaks the drivers of vehicles need to have\", type=\"string\")\n",
    "    response_schemas = [tw_schema, dimension_schema, skill_schema, break_schema]\n",
    "    \n",
    "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    template_string = \"\"\"Extract the information of a vehicle routing prblem from the text \\\n",
    "    that is delimited by triple backticks.\n",
    "        \n",
    "    The information you need to extract are:\n",
    "    \n",
    "    - the number of time windows. \n",
    "    - the number dimension of the loads and the resource (vehicle) capacity. \n",
    "    - the number of skills that the loads and vehicles posess. \n",
    "    - the maximum number of breaks the drivers of vehicles need to have. \n",
    "\n",
    "    text: ```{text}```\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_templete = ChatPromptTemplate.from_template(template=template_string)\n",
    "    message = prompt_templete.format_messages(text=user_message, format_instructions=format_instructions)\n",
    "\n",
    "    chat = OpenAI(temperature=0.1, base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\", model=\"local-model\")\n",
    "    response = chat.invoke(message)\n",
    "    result = output_parser.invoke(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75ac96-b301-4eb4-98f4-28998a4ca26d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:175\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:157\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:125\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI, OpenAI\n\u001b[1;32m      3\u001b[0m user_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mWe have three time windows and 4 breaks\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mextract_vrp_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_message\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m      9\u001b[0m user_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThe dimension of our load is 4 or 5. The number of skills depends on the situation but it is around 2\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[24], line 33\u001b[0m, in \u001b[0;36mextract_vrp_info\u001b[0;34m(user_message)\u001b[0m\n\u001b[1;32m     31\u001b[0m chat \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:1234/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot-needed\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal-model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m response \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39minvoke(message)\n\u001b[0;32m---> 33\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:176\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:1246\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1243\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1244\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1245\u001b[0m         Output,\n\u001b[0;32m-> 1246\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1254\u001b[0m     )\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1256\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:177\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 177\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    179\u001b[0m         config,\n\u001b[1;32m    180\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:219\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m        Structured output.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain/output_parsers/structured.py:97\u001b[0m, in \u001b[0;36mStructuredOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     96\u001b[0m     expected_keys \u001b[38;5;241m=\u001b[39m [rs\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_schemas]\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:177\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    175\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m parse_json_markdown(text)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid JSON object. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_obj:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "\n",
    "user_message = \"\"\"We have three time windows and 4 breaks\"\"\"\n",
    "\n",
    "result = extract_vrp_info(user_message)   \n",
    "\n",
    "print(result)\n",
    "\n",
    "user_message = \"\"\"The dimension of our load is 4 or 5. The number of skills depends on the situation but it is around 2\"\"\"\n",
    "\n",
    "result = extract_vrp_info(user_message)   \n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3acd47b-2573-4861-8663-8fbab7ff00e3",
   "metadata": {},
   "source": [
    "#### クライアントがどのサプライチェーン最適化モデルに興味があるのかを聞き出すプロンプトを生成する関数 extract_sc_models\n",
    "\n",
    "引数：\n",
    "\n",
    "- user_message: ユーザーメッセージ\n",
    "\n",
    "返値：\n",
    "\n",
    "- message: LLMに入れる文字列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4c870-a868-43a0-a49d-55ee1665540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_sc_models(user_message:str) -> str: \n",
    "\n",
    "    category_schema = ResponseSchema(name=\"category\", description=\"category that user is interested in\", type=\"float\")\n",
    "    confidense_schema = ResponseSchema(name=\"confidense\", description=\"confidense prbability\", type=\"float\")\n",
    "    response_schemas = [category_schema, confidense_schema]\n",
    "    \n",
    "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    template_string = \"\"\" \n",
    "    You will be provided with customer queries. \\\n",
    "    The customer query will be delimited with #### characters.\\\n",
    "    \n",
    "    Output a python list of objects, where each object has \\\n",
    "    the following format:\n",
    "        'category': <one of Logistcs or Supply Chain Network Design, Vehicle Routing, \\\n",
    "        Inventory Planning and Optimization, \\\n",
    "        Production Scheduling, Demand Forecasting>,\n",
    "    \n",
    "    Each category has the following definition and keywords.\n",
    "    The categories or keywards must be found in the customer query.\n",
    "    \n",
    "    Allowed categories: \n",
    "    \n",
    "    Logistcs or Supply Chain Network Design:\n",
    "    \n",
    "    - definition: \n",
    "    A supply-chain network can be strategically designed in such a way as to \\ \n",
    "    reduce the cost of the supply chain; it has been suggested by experts  |\n",
    "    that 80% of supply chain costs are determined by location of facilities and the flow of product \\\n",
    "    between the facilities. \\\n",
    "    Supply chain network design is sometimes referred to as 'Network Modelling', \\ \n",
    "    due to the fact a mathematical model can be created to optimize the supply-chain network. \\\n",
    "    Companies have been led to modify their basic supply chain,  \\ \n",
    "    investing in the tools and resources to develop an improved SCN \\ \n",
    "    design that takes into account taxation regulations, new entrants  \\\n",
    "    into their industry and availability of resources, has resulted in more complex network designs. \\\n",
    "    Designing a SCN involves creating a network that incorporates all the facilities,  \\ \n",
    "    means of production, products, and transportation assets owned by the organization \\ \n",
    "    or those not owned by the organization but which immediately support the supply-chain  \\ \n",
    "    operations and product flow. The design should also include details of the number and \\ \n",
    "    location of facilities: plants, warehouses, and supplier base. \n",
    "    \n",
    "    - keywards: facility, location, relocation, assignment of product, long-term decision, strategic level\n",
    "    \n",
    "    Vehicle Routing:\n",
    "    \n",
    "    - definition: \n",
    "    The VRP concerns the service of a delivery company. \\\n",
    "    How things are delivered from one or more depots which has a given set of home vehicles \\\n",
    "    and operated by a set of drivers who can move on a given road network to a set of customers. \\\n",
    "    It asks for a determination of a set of routes, S, (one route for each vehicle that must \\\n",
    "    start and finish at its own depot) such that all customers' requirements and operational \\\n",
    "    constraints are satisfied and the global transportation cost is minimized.  \\\n",
    "    This cost may be monetary, distance or otherwise.\n",
    "    \n",
    "    - keywards: vehicle, truck, milkrun, time windows, ship, transportation, backhaul, \\ \n",
    "    routing, operational, short-term decision\n",
    "    \n",
    "    Inventory Planning:\n",
    "    \n",
    "    - keywards: inventory, shelf, safet stock, cycle stock, allocation, \\\n",
    "      operational and tactical, mid-term decision\n",
    "    \n",
    "    Production Scheduling:\n",
    "    \n",
    "    - keywards: plant, scheduling, lot-size, sequence, resource assignment, operational and tactical, mid-term decision\n",
    "    \n",
    "    Demand Forecasting:\n",
    "    \n",
    "    - keywards: predict, forecast, demand, product, machine learning, deep learning\n",
    "\n",
    "\n",
    "    ####{text}####\n",
    "\n",
    "    Outout category and confidense using {format_instructions}\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_templete = ChatPromptTemplate.from_template(template=template_string)\n",
    "    message = prompt_templete.format_messages(text=user_message, format_instructions=format_instructions)\n",
    "\n",
    "    chat = OpenAI(temperature=0.1, base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\", model=\"local-model\")\n",
    "    response = chat.invoke(message)\n",
    "    result = output_parser.invoke(response)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1984f603-5c92-45e5-8daf-3833ddf5e4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 2.0, 'confidense': 0.9}\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:175\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:157\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:125\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/json/__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m      8\u001b[0m user_message_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m we are considering to select warehouses and plants for a new product \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m result  \u001b[38;5;241m=\u001b[39m \u001b[43mextract_sc_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_message_2\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[26], line 86\u001b[0m, in \u001b[0;36mextract_sc_models\u001b[0;34m(user_message)\u001b[0m\n\u001b[1;32m     84\u001b[0m chat \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:1234/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot-needed\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal-model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m response \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39minvoke(message)\n\u001b[0;32m---> 86\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:176\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:1246\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1243\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1244\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1245\u001b[0m         Output,\n\u001b[0;32m-> 1246\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1254\u001b[0m     )\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1256\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:177\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 177\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    179\u001b[0m         config,\n\u001b[1;32m    180\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:219\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m        Structured output.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain/output_parsers/structured.py:97\u001b[0m, in \u001b[0;36mStructuredOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     96\u001b[0m     expected_keys \u001b[38;5;241m=\u001b[39m [rs\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_schemas]\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:177\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    175\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m parse_json_markdown(text)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid JSON object. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_obj:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "user_message_1 = f\"\"\"\n",
    " tell me about the vehicle scheduling and optimization. \\\n",
    " Also tell me about your forecasting systems \"\"\"\n",
    "\n",
    "result  = extract_sc_models(user_message_1)   \n",
    "print(result)\n",
    "\n",
    "user_message_2 = f\"\"\"\n",
    " we are considering to select warehouses and plants for a new product \"\"\"\n",
    "\n",
    "result  = extract_sc_models(user_message_2)   \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ba71e-925d-4f13-b14b-564b3bddd82c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid return object. Expected key `confidense` to be present, but got {'category': 1.0, 'confidence': 0.8}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m user_message_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mThank you for reaching out! Our current transportation and delivery process is structured around a centralized dispatch system that assigns deliveries to drivers based on their location and availability. We use a combination of manual and automated route planning tools to optimize our delivery routes, but we often encounter challenges such as traffic congestion, road closures, and unpredictable weather conditions.\u001b[39m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mOur main objective for enhancing our delivery optimization is to reduce delivery times and improve customer satisfaction. We would like to explore more advanced route planning tools and strategies that can help us better anticipate and respond to changing conditions on the road, as well as identify opportunities for consolidating deliveries and reducing overall transportation costs.\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mextract_sc_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_message_3\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[26], line 86\u001b[0m, in \u001b[0;36mextract_sc_models\u001b[0;34m(user_message)\u001b[0m\n\u001b[1;32m     84\u001b[0m chat \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:1234/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot-needed\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal-model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m response \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39minvoke(message)\n\u001b[0;32m---> 86\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:176\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:1246\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1243\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1244\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1245\u001b[0m         Output,\n\u001b[0;32m-> 1246\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1254\u001b[0m     )\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1256\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:177\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 177\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    179\u001b[0m         config,\n\u001b[1;32m    180\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:219\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m        Structured output.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain/output_parsers/structured.py:97\u001b[0m, in \u001b[0;36mStructuredOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     96\u001b[0m     expected_keys \u001b[38;5;241m=\u001b[39m [rs\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_schemas]\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/dev/scmopt2/env/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:180\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_obj:\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m    181\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid return object. Expected key `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be present, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m         )\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json_obj\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid return object. Expected key `confidense` to be present, but got {'category': 1.0, 'confidence': 0.8}"
     ]
    }
   ],
   "source": [
    "user_message_3 = \"\"\"\n",
    "Thank you for reaching out! Our current transportation and delivery process is structured around a centralized dispatch system that assigns deliveries to drivers based on their location and availability. We use a combination of manual and automated route planning tools to optimize our delivery routes, but we often encounter challenges such as traffic congestion, road closures, and unpredictable weather conditions.\n",
    "\n",
    "In terms of tracking, we currently use GPS-enabled devices on our vehicles to monitor their location and progress throughout the day. We also have a web-based dashboard that provides real-time visibility into our delivery operations, including estimated arrival times and delivery status updates.\n",
    "\n",
    "Our main objective for enhancing our delivery optimization is to reduce delivery times and improve customer satisfaction. We would like to explore more advanced route planning tools and strategies that can help us better anticipate and respond to changing conditions on the road, as well as identify opportunities for consolidating deliveries and reducing overall transportation costs.\n",
    "\"\"\"\n",
    "result = extract_sc_models(user_message_3)   \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340bf39-77eb-4ab2-bcb5-9f3c3d3d96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "opening_message = \"\"\"\n",
    "Hello, I am FuturoFlow, the AI assistant for supply chain optimization. 🚀 \\\n",
    "\n",
    "FuturoFlow: An AI tool driving future-oriented supply chain processes. \\\n",
    "\n",
    "I respond to various challenges and questions regarding supply chain efficiency,  \\\n",
    "providing effective solutions and assisting in maximizing business outcomes.    \\\n",
    "\n",
    "Please tell us about the challenges or difficulties you are facing regarding your company's supply chain.\n",
    "\"\"\"\n",
    "\n",
    "vrp_message = \"\"\"\n",
    "Hello! We're eager to gain a better understanding of your delivery planning requirements. \\\\\n",
    "To tailor our solutions effectively, could you please provide the following details:\n",
    "\n",
    "Number of Time Windows:\n",
    "\n",
    "- When considering deliveries to customers, how many time slots (time windows) are available for truck visits? \\\n",
    "For example, if the truck can visit between 10:00 AM to 12:00 PM and then between 1:00 PM to 3:00 PM with a break for lunch, you would have two time windows: [10:00, 12:00] and [13:00, 15:00].\n",
    "\n",
    "Number of Dimensions of Cargo:\n",
    "\n",
    "- How many units are used to determine if the cargo can be loaded onto the truck? \\\n",
    "For example, if you need to consider weight, volume, and quantity, the number of dimensions of the cargo would be 3.\n",
    "\n",
    "Number of Skills:\n",
    "\n",
    "- The concept of \"skills\" is used to represent the compatibility between customers (locations) and trucks (drivers) for delivery. \\\n",
    "If a truck does not possess all the skills required by a customer, it cannot handle that customer's delivery. \\\n",
    "For instance, if there are customers who can only accommodate trucks with a capacity of less than 10 tons, you would define a skill for trucks with a capacity of less than 10 tons and ensure that trucks with a capacity of 10 tons or more do not possess this skill. In this case, the number of skills would be 1. How many skills does your company anticipate needing?\n",
    "\n",
    "Number of Breaks:\n",
    "\n",
    "- How many breaks can a truck (driver) take? \\\n",
    "For example, if there is a possibility of taking breaks during lunch and dinner times, the number would be 2.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee685c-208e-4645-aec8-f241e94a2f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "def predict(message, history):\n",
    "    history_openai_format = []\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human })\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n",
    "    if len(history)==0:\n",
    "        state = \"start\"\n",
    "    if state == \"start\":\n",
    "        history_openai_format = [ {'role':'system', 'content': system_message} ]\n",
    "    elif state == \"vrp\":\n",
    "        history_openai_format = [ {'role':'system', 'content': vrp_message} ] #vrp message\n",
    "        \n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"local-model\", \n",
    "        messages=history_openai_format,\n",
    "        temperature=0.\n",
    "    )\n",
    "\n",
    "    res_json = response.choices[0].message.content\n",
    "    print( res_json )\n",
    "    res = json.loads(res_json[8:-4])[0][\"category\"]\n",
    "\n",
    "    if res == \"Logistcs or Supply Chain Network Design\":\n",
    "        pass\n",
    "    elif res == \"Vehicle Routing\":\n",
    "        state =\"vrp\"\n",
    "    elif res == \"Inventory Planning and Optimization\":\n",
    "        pass\n",
    "    elif res == \"Production Scheduling\":\n",
    "        pass\n",
    "    elif res == \"Demand Forecasting\":\n",
    "        pass\n",
    "    \n",
    "    yield f\"You are interested in {res}! \"\n",
    "\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d56b68d-7368-4bf8-916e-665a9b736090",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenAI' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m history \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an intelligent assistant. You always provide well-reasoned answers that are both correct and helpful.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     11\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, introduce yourself to someone opening this program for the first time. Be concise.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     16\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal-model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# this field is currently unused\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         messages\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m     18\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     19\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     new_message \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m completion:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenAI' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "\n",
    "# from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant. You always provide well-reasoned answers that are both correct and helpful.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, introduce yourself to someone opening this program for the first time. Be concise.\"},\n",
    "]\n",
    "    \n",
    "while True:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"local-model\", # this field is currently unused\n",
    "        messages=history,\n",
    "        temperature=0.7,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    \n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "            new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "    history.append(new_message)\n",
    "    \n",
    "    # Uncomment to see chat history\n",
    "    # import json\n",
    "    # gray_color = \"\\033[90m\"\n",
    "    # reset_color = \"\\033[0m\"\n",
    "    # print(f\"{gray_color}\\n{'-'*20} History dump {'-'*20}\\n\")\n",
    "    # print(json.dumps(history, indent=2))\n",
    "    # print(f\"\\n{'-'*55}\\n{reset_color}\")\n",
    "\n",
    "    print()\n",
    "    greeting_message = input(\"> \")\n",
    "    history.append({\"role\": \"user\", \"content\": greeting_message}) #入力\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Tell me which language this is: \n",
    "    ```{greeting_message}```\n",
    "    Use the following format:\n",
    "    language: <language name>\n",
    "    \"\"\"\n",
    "    language = get_completion(prompt)\n",
    "    language = language.split(\":\")[1]\n",
    "    print(\"Language:\", language) #これを記憶しておいて，ユーザーへの返答はこの言語に翻訳してから返す！\n",
    "\n",
    "    #どのモデルを使うかクライアントに聞く\n",
    "    prompt = f\"\"\"\n",
    "    Translate the following  text to {language}: `どのような問題でお困りですか?`\n",
    "    \"\"\"\n",
    "    print(get_completion(prompt))\n",
    "    \n",
    "    issue = input(\"> \")\n",
    "    \n",
    "    #英語に翻訳してから入れる\n",
    "    prompt = f\"\"\"\n",
    "    Translate the following  text to English: ```{issue}```\n",
    "    \"\"\"\n",
    "    user_message = get_completion(prompt)\n",
    "    #print(user_message, \"\\n\")\n",
    "    \n",
    "    messages =  [  \n",
    "    {'role':'system', \n",
    "     'content': system_message},    \n",
    "    {'role':'user', \n",
    "     'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "    ] \n",
    "    response = get_completion_from_messages(messages)\n",
    "    print(\"Classification\", response)\n",
    "    \n",
    "    #分類に基づいて分岐し，モデルの詳細を聞く\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff80910-c00e-4e98-b00d-7dc02d95777c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.16.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/gradio/blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/gradio/blocks.py\", line 1191, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/gradio/utils.py\", line 521, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/gradio/utils.py\", line 645, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/gradio/chat_interface.py\", line 487, in _stream_fn\n",
      "    first_response = await async_iteration(generator)\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/gradio/utils.py\", line 521, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/gradio/utils.py\", line 514, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/anyio/to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/mikiokubo/Documents/dev/scmopt2/env/lib/python3.10/site-packages/gradio/utils.py\", line 497, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"/var/folders/c0/1l3dlhys02nbkv4fyj9pkm680000gn/T/ipykernel_83161/1583975101.py\", line 14, in predict\n",
      "    history_openai_format = [ {'role':'system', 'content': system_message} ]\n",
      "NameError: name 'system_message' is not defined. Did you mean: 'user_message'?\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(temperature= 0.0, base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n",
    "\n",
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n",
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n",
    "\n",
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced556a-ffe0-489d-90ed-99a966757ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
