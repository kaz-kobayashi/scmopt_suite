import numpy as np
import pandas as pd
from scipy import stats
from scipy.stats import norm
import networkx as nx
from collections import defaultdict
import warnings
from typing import Tuple, Optional, List, Dict, Any

warnings.filterwarnings('ignore')

def eoq(K: float, d: float, h: float, b: float = 0, r: float = 0, c: float = 1, theta: float = 1, 
        discount: Optional[List[Tuple[float, float]]] = None,
        discount_type: str = "incremental",
        allow_backorder: bool = False) -> Dict[str, Any]:
    """
    Advanced Economic Order Quantity model solver
    Exact implementation from notebook supporting all EOQ variants
    
    Args:
        K: Fixed ordering cost
        d: Demand rate per period
        h: Holding cost per unit per period
        b: Backorder cost per unit per period (if allow_backorder=True)
        r: Interest rate
        c: Unit cost
        theta: Service level (fill rate)
        discount: Quantity discount schedule [(breakpoint, unit_price)]
        discount_type: "incremental" or "all_units" discount
        allow_backorder: Whether backorders are allowed
    
    Returns:
        Dictionary with optimal solution and detailed analysis
    """
    results = {
        'model_type': 'basic_eoq',
        'parameters': {
            'fixed_cost': K,
            'demand_rate': d,
            'holding_cost': h,
            'backorder_cost': b,
            'interest_rate': r,
            'unit_cost': c,
            'service_level': theta,
            'allow_backorder': allow_backorder,
            'discount_type': discount_type
        }
    }
    
    if discount is None:
        # Standard EOQ or EOQ with backorders
        if allow_backorder and b > 0:
            # EOQ with planned backorders
            Q_star = np.sqrt(2 * K * d * (h + b) / (h * b))
            S_star = Q_star * b / (h + b)  # Maximum shortage
            B_star = Q_star * h / (h + b)  # Maximum backorder level
            
            # Total relevant cost
            TC = K * d / Q_star + h * S_star**2 / (2 * Q_star) + b * B_star**2 / (2 * Q_star)
            
            # Fill rate
            fill_rate = S_star / Q_star
            
            results.update({
                'model_type': 'eoq_with_backorders',
                'optimal_order_quantity': float(Q_star),
                'optimal_shortage_level': float(S_star),
                'max_backorder_level': float(B_star),
                'total_relevant_cost': float(TC),
                'annual_ordering_cost': float(K * d / Q_star),
                'annual_holding_cost': float(h * S_star**2 / (2 * Q_star)),
                'annual_backorder_cost': float(b * B_star**2 / (2 * Q_star)),
                'fill_rate': float(fill_rate),
                'cycle_time': float(Q_star / d),
                'service_level_achieved': float(fill_rate)
            })
        else:
            # Basic EOQ without backorders
            Q_star = np.sqrt(2 * K * d / h)
            TC = np.sqrt(2 * K * d * h)
            
            results.update({
                'model_type': 'basic_eoq',
                'optimal_order_quantity': float(Q_star),
                'total_relevant_cost': float(TC),
                'annual_ordering_cost': float(K * d / Q_star),
                'annual_holding_cost': float(h * Q_star / 2),
                'cycle_time': float(Q_star / d),
                'service_level_achieved': 1.0
            })
    
    else:
        # EOQ with quantity discounts
        results['model_type'] = f'eoq_with_{discount_type}_discounts'
        results['discount_schedule'] = discount
        
        # Sort discount schedule by breakpoint
        discount_sorted = sorted(discount, key=lambda x: x[0])
        
        candidate_solutions = []
        
        if discount_type == "incremental":
            # Incremental quantity discount
            for i, (breakpoint, unit_price) in enumerate(discount_sorted):
                # Calculate effective unit cost considering incremental pricing
                if i == 0:
                    effective_cost = unit_price
                else:
                    # Weighted average cost up to this breakpoint
                    total_cost = 0
                    prev_breakpoint = 0
                    
                    for j in range(i + 1):
                        current_breakpoint, current_price = discount_sorted[j]
                        if j == 0:
                            qty_in_bracket = min(breakpoint, current_breakpoint)
                        else:
                            qty_in_bracket = min(breakpoint, current_breakpoint) - prev_breakpoint
                        
                        total_cost += qty_in_bracket * current_price
                        prev_breakpoint = current_breakpoint
                    
                    effective_cost = total_cost / breakpoint if breakpoint > 0 else unit_price
                
                # Adjusted holding cost
                h_adj = h * effective_cost / c if c > 0 else h
                
                # EOQ calculation
                Q_optimal = np.sqrt(2 * K * d / h_adj)
                Q_feasible = max(Q_optimal, breakpoint)
                
                # Calculate actual unit cost for this quantity
                actual_unit_cost = 0
                remaining_qty = Q_feasible
                cost_accumulator = 0
                
                for j, (bp, price) in enumerate(discount_sorted):
                    if remaining_qty <= 0:
                        break
                    
                    if j == 0:
                        qty_in_bracket = min(remaining_qty, bp)
                    else:
                        prev_bp = discount_sorted[j-1][0]
                        qty_in_bracket = min(remaining_qty, bp - prev_bp)
                    
                    cost_accumulator += qty_in_bracket * price
                    remaining_qty -= qty_in_bracket
                
                if remaining_qty > 0:
                    # Use the last price tier
                    cost_accumulator += remaining_qty * discount_sorted[-1][1]
                
                actual_unit_cost = cost_accumulator / Q_feasible
                
                # Total cost calculation
                TC = K * d / Q_feasible + h * actual_unit_cost * Q_feasible / (2 * c) + actual_unit_cost * d
                
                candidate_solutions.append({
                    'order_quantity': Q_feasible,
                    'unit_cost': actual_unit_cost,
                    'total_cost': TC,
                    'price_tier': i,
                    'breakpoint': breakpoint
                })
        
        else:
            # All-units quantity discount
            for i, (breakpoint, unit_price) in enumerate(discount_sorted):
                # Adjusted holding cost
                h_adj = h * unit_price / c if c > 0 else h
                
                # EOQ calculation
                Q_optimal = np.sqrt(2 * K * d / h_adj)
                Q_feasible = max(Q_optimal, breakpoint)
                
                # Total cost with all-units discount
                TC = K * d / Q_feasible + h_adj * Q_feasible / 2 + unit_price * d
                
                candidate_solutions.append({
                    'order_quantity': Q_feasible,
                    'unit_cost': unit_price,
                    'total_cost': TC,
                    'price_tier': i,
                    'breakpoint': breakpoint
                })
        
        # Select best solution
        best_solution = min(candidate_solutions, key=lambda x: x['total_cost'])
        
        Q_star = best_solution['order_quantity']
        unit_cost_final = best_solution['unit_cost']
        TC_final = best_solution['total_cost']
        
        results.update({
            'optimal_order_quantity': float(Q_star),
            'optimal_unit_cost': float(unit_cost_final),
            'total_relevant_cost': float(TC_final),
            'annual_ordering_cost': float(K * d / Q_star),
            'annual_holding_cost': float(h * unit_cost_final * Q_star / (2 * c)),
            'annual_purchase_cost': float(unit_cost_final * d),
            'selected_price_tier': int(best_solution['price_tier']),
            'selected_breakpoint': float(best_solution['breakpoint']),
            'cycle_time': float(Q_star / d),
            'service_level_achieved': 1.0,
            'all_candidates': candidate_solutions,
            'savings_vs_no_discount': float(candidate_solutions[0]['total_cost'] - TC_final) if len(candidate_solutions) > 1 else 0
        })
    
    return results

def simulate_inventory(n_samples: int, n_periods: int, mu: float, sigma: float,
                      LT: int, Q: float, R: float, b: float, h: float, fc: float,
                      S: Optional[float] = None) -> Tuple[np.ndarray, np.ndarray]:
    """
    Simulate (Q,R) or (s,S) inventory policies
    
    Args:
        n_samples: Number of simulation runs
        n_periods: Number of periods per run
        mu: Mean demand per period
        sigma: Standard deviation of demand
        LT: Lead time in periods
        Q: Order quantity (for Q,R policy)
        R: Reorder point
        b: Backorder cost per unit
        h: Holding cost per unit per period
        fc: Fixed ordering cost
        S: Order-up-to level (for s,S policy, optional)
    
    Returns:
        Tuple of (costs, inventory_levels)
    """
    costs = np.zeros(n_samples)
    inventory_histories = []
    
    for sample in range(n_samples):
        # Initialize
        inventory = Q  # Starting inventory
        on_order = 0
        total_cost = 0
        inventory_history = [inventory]
        orders_pending = []  # [(arrival_period, quantity)]
        
        for period in range(n_periods):
            # Receive orders that arrive this period
            arriving_orders = [qty for arr_period, qty in orders_pending if arr_period == period]
            inventory += sum(arriving_orders)
            orders_pending = [(arr, qty) for arr, qty in orders_pending if arr != period]
            on_order -= sum(arriving_orders)
            
            # Generate demand
            demand = max(0, np.random.normal(mu, sigma))
            
            # Meet demand
            if inventory >= demand:
                inventory -= demand
                holding_cost = h * inventory
                shortage_cost = 0
            else:
                shortage = demand - inventory
                inventory = 0
                holding_cost = 0
                shortage_cost = b * shortage
            
            total_cost += holding_cost + shortage_cost
            
            # Check for reorder
            if S is not None:
                # (s,S) policy
                if inventory + on_order <= R:
                    order_qty = S - inventory - on_order
                    if order_qty > 0:
                        total_cost += fc
                        orders_pending.append((period + LT, order_qty))
                        on_order += order_qty
            else:
                # (Q,R) policy
                if inventory + on_order <= R:
                    total_cost += fc
                    orders_pending.append((period + LT, Q))
                    on_order += Q
            
            inventory_history.append(inventory)
        
        costs[sample] = total_cost / n_periods  # Average cost per period
        inventory_histories.append(inventory_history)
    
    return costs, np.array(inventory_histories)

def optimize_qr(n_samples: int, n_periods: int, mu: float, sigma: float,
                LT: int, Q: float, R: float, z: float, b: float, h: float,
                fc: float, alpha: float = 0.95) -> Tuple[float, float]:
    """
    Optimize (Q,R) policy parameters
    
    Args:
        n_samples: Number of simulation samples
        n_periods: Number of periods
        mu: Mean demand
        sigma: Standard deviation of demand  
        LT: Lead time
        Q: Initial order quantity
        R: Initial reorder point
        z: Safety factor
        b: Backorder cost
        h: Holding cost
        fc: Fixed cost
        alpha: Service level
    
    Returns:
        Tuple of optimal (R, Q)
    """
    # Use newsvendor model for reorder point
    demand_LT_mean = mu * LT
    demand_LT_std = sigma * np.sqrt(LT)
    
    # Critical ratio for service level
    critical_ratio = alpha
    R_optimal = demand_LT_mean + stats.norm.ppf(critical_ratio) * demand_LT_std
    
    # EOQ for order quantity
    Q_optimal = np.sqrt(2 * fc * mu / h)
    
    return R_optimal, Q_optimal

def approximate_ss(mu: float, sigma: float, LT: int, b: float, h: float, fc: float) -> Tuple[float, float]:
    """
    Approximate optimal (s,S) parameters using analytical methods
    
    Args:
        mu: Mean demand per period
        sigma: Standard deviation of demand
        LT: Lead time
        b: Backorder cost
        h: Holding cost  
        fc: Fixed ordering cost
    
    Returns:
        Tuple of (s, S) parameters
    """
    # Use approximation formulas
    EOQ = np.sqrt(2 * fc * mu / h)
    
    # Safety stock calculation
    demand_LT_mean = mu * LT
    demand_LT_std = sigma * np.sqrt(LT)
    
    # Critical ratio
    critical_ratio = b / (b + h)
    safety_stock = stats.norm.ppf(critical_ratio) * demand_LT_std
    
    s = demand_LT_mean + safety_stock
    S = s + EOQ
    
    return s, S

def base_stock_simulation(n_samples: int, n_periods: int, demand: np.ndarray,
                         capacity: float, LT: int, b: float, h: float,
                         S: float) -> Tuple[float, float, np.ndarray]:
    """
    Simulate periodic review base-stock policy
    
    Args:
        n_samples: Number of simulation runs
        n_periods: Number of periods
        demand: Demand realizations
        capacity: Production capacity
        LT: Lead time
        b: Backorder cost
        h: Holding cost
        S: Base stock level
    
    Returns:
        Tuple of (derivative, total_cost, inventory_levels)
    """
    total_costs = []
    all_inventory = []
    
    for sample in range(n_samples):
        inventory = S
        total_cost = 0
        inventory_history = [inventory]
        
        for period in range(min(n_periods, len(demand))):
            # Meet demand
            period_demand = demand[period]
            
            if inventory >= period_demand:
                inventory -= period_demand
                shortage_cost = 0
            else:
                shortage = period_demand - inventory
                inventory = 0
                shortage_cost = b * shortage
            
            # Holding cost
            holding_cost = h * max(0, inventory)
            total_cost += holding_cost + shortage_cost
            
            # Replenishment (simplified)
            replenish = min(capacity, max(0, S - inventory))
            inventory += replenish
            
            inventory_history.append(inventory)
        
        total_costs.append(total_cost)
        all_inventory.append(inventory_history)
    
    avg_cost = np.mean(total_costs)
    derivative = 0  # Simplified - would need numerical differentiation
    
    return derivative, avg_cost, np.array(all_inventory)

def multi_echelon_optimization(demand_data: np.ndarray, 
                              network_structure: Dict[str, Any],
                              cost_parameters: Dict[str, float]) -> Dict[str, Any]:
    """
    Multi-echelon inventory optimization
    
    Args:
        demand_data: Historical demand data
        network_structure: Network topology and parameters
        cost_parameters: Cost structure (holding, ordering, shortage costs)
    
    Returns:
        Optimized inventory policies for each echelon
    """
    # Simplified multi-echelon optimization
    echelons = network_structure.get('echelons', ['plant', 'dc', 'retail'])
    results = {}
    
    for i, echelon in enumerate(echelons):
        # Calculate demand statistics
        demand_mean = np.mean(demand_data)
        demand_std = np.std(demand_data)
        
        # Adjust for echelon level (upstream has higher variability)
        level_multiplier = (len(echelons) - i) / len(echelons)
        adjusted_std = demand_std * level_multiplier
        
        # Calculate inventory parameters
        holding_cost = cost_parameters.get(f'{echelon}_holding_cost', 1.0)
        ordering_cost = cost_parameters.get(f'{echelon}_ordering_cost', 100.0)
        
        # EOQ calculation
        eoq = np.sqrt(2 * ordering_cost * demand_mean / holding_cost)
        
        # Safety stock calculation
        service_level = cost_parameters.get('service_level', 0.95)
        z_score = 1.65 if service_level == 0.95 else 2.33  # Simplified
        safety_stock = z_score * adjusted_std
        
        # Reorder point
        lead_time = network_structure.get(f'{echelon}_lead_time', 1)
        reorder_point = demand_mean * lead_time + safety_stock
        
        results[echelon] = {
            'eoq': float(eoq),
            'safety_stock': float(safety_stock),
            'reorder_point': float(reorder_point),
            'target_stock': float(reorder_point + eoq),
            'demand_mean': float(demand_mean),
            'demand_std': float(adjusted_std),
            'cost_parameters': {
                'holding_cost': holding_cost,
                'ordering_cost': ordering_cost
            }
        }
    
    return {
        'echelon_policies': results,
        'total_cost_estimate': sum(
            policy['eoq'] * cost_parameters.get(f"{echelon}_holding_cost", 1.0) / 2 +
            demand_mean * cost_parameters.get(f"{echelon}_ordering_cost", 100.0) / policy['eoq']
            for echelon, policy in results.items()
        ),
        'network_structure': network_structure
    }

def inventory_abc_classification(demand_df: pd.DataFrame, 
                                inventory_costs: Dict[str, float]) -> Dict[str, Any]:
    """
    ABC classification specifically for inventory management
    
    Args:
        demand_df: Demand data with product information
        inventory_costs: Cost per unit for each product
    
    Returns:
        ABC classification with inventory management recommendations
    """
    # Calculate annual demand and costs
    annual_demand = demand_df.groupby('prod')['demand'].sum()
    
    inventory_value = {}
    for prod in annual_demand.index:
        unit_cost = inventory_costs.get(prod, 1.0)
        inventory_value[prod] = annual_demand[prod] * unit_cost
    
    # Create DataFrame for analysis
    analysis_df = pd.DataFrame({
        'product': list(inventory_value.keys()),
        'annual_demand': [annual_demand[p] for p in inventory_value.keys()],
        'unit_cost': [inventory_costs.get(p, 1.0) for p in inventory_value.keys()],
        'inventory_value': list(inventory_value.values())
    })
    
    # Sort by inventory value
    analysis_df = analysis_df.sort_values('inventory_value', ascending=False)
    total_value = analysis_df['inventory_value'].sum()
    
    # Calculate cumulative percentages
    analysis_df['cumsum'] = analysis_df['inventory_value'].cumsum()
    analysis_df['cum_pct'] = analysis_df['cumsum'] / total_value
    
    # ABC Classification
    analysis_df['abc_class'] = 'C'
    analysis_df.loc[analysis_df['cum_pct'] <= 0.8, 'abc_class'] = 'A'
    analysis_df.loc[(analysis_df['cum_pct'] > 0.8) & (analysis_df['cum_pct'] <= 0.95), 'abc_class'] = 'B'
    
    # Management recommendations
    recommendations = {
        'A': {
            'inventory_policy': 'Tight control, frequent reviews, EOQ with safety stock',
            'review_frequency': 'Weekly',
            'service_level': 0.98,
            'management_attention': 'High'
        },
        'B': {
            'inventory_policy': 'Moderate control, periodic reviews, standard EOQ',
            'review_frequency': 'Monthly', 
            'service_level': 0.95,
            'management_attention': 'Medium'
        },
        'C': {
            'inventory_policy': 'Loose control, bulk orders, min-max system',
            'review_frequency': 'Quarterly',
            'service_level': 0.90,
            'management_attention': 'Low'
        }
    }
    
    return {
        'classification_data': analysis_df.to_dict('records'),
        'summary': {
            'total_products': len(analysis_df),
            'total_inventory_value': float(total_value),
            'class_distribution': analysis_df['abc_class'].value_counts().to_dict(),
            'class_value_distribution': analysis_df.groupby('abc_class')['inventory_value'].sum().to_dict()
        },
        'recommendations': recommendations
    }

# Advanced Inventory Management Functions from 03inventory.ipynb

def ww(demand: List[float], fc: float = 100., vc: float = 0., h: float = 5.) -> Tuple[float, np.ndarray]:
    """
    Wagner-Whitin dynamic lot sizing algorithm
    Exact implementation from notebook for dynamic lot sizing problems
    
    Args:
        demand: Multi-period demand list
        fc: Fixed cost (constant or list)
        vc: Variable cost (constant or list) 
        h: Holding cost (constant or list)
        
    Returns:
        cost: Optimal value
        order: Order quantities for each period
    """
    T = len(demand)
    fixed = np.full(T, fc) 
    variable = np.full(T, vc)
    hc = np.full(T, h)
    F = np.full(T, 99999999999.)

    prev = np.full(T, -1)
    for i in range(T):
        if i == 0: 
            cum = fixed[i] + variable[i] * demand[i]
        else:
            cum = F[i-1] + fixed[i] + variable[i] * demand[i]
        cumh = 0
        for j in range(i, T):
            if cum < F[j]:
                F[j] = cum
                prev[j] = i - 1
            if j == (T - 1): 
                break
            cumh += hc[j]
            cum += (variable[i] + cumh) * demand[j+1]

    setup = np.zeros(T)
    j = T - 1
    while j != -1:
        i = prev[j]
        setup[i+1] = 1
        j = i

    dem = 0
    order = np.zeros(T)
    for t in range(T-1, -1, -1):
        dem += demand[t]
        if setup[t] == 1:
            order[t] = dem
            dem = 0
            
    return F[T-1], order

def best_distribution(data: np.ndarray) -> Tuple[object, object, str, tuple]:
    """
    Find best fitting continuous probability distribution for stationary demand data
    
    Args:
        data: Array of demand data
        
    Returns:
        fig: Histogram and best fit distribution density function plot
        frozen_dist: Best fit distribution (with fixed parameters)
        best_fit_name: Name of best fit distribution
        best_fit_params: Parameters of best fit distribution
    """
    import scipy.stats as st
    
    def best_fit_distribution(data, bins=200):
        """Model data by finding best fit distribution to data"""
        # Distributions to check
        DISTRIBUTIONS = [        
            st.alpha, st.anglit, st.arcsine, st.beta, st.betaprime, st.bradford, st.burr, st.cauchy, st.chi, st.chi2, st.cosine,
            st.dgamma, st.dweibull, st.expon, st.exponnorm, st.exponweib, st.exponpow, st.f, st.fatiguelife, st.fisk,
            st.foldcauchy, st.foldnorm, st.genlogistic, st.genpareto, st.gennorm, st.genexpon,
            st.genextreme, st.gausshyper, st.gamma, st.gengamma, st.genhalflogistic, st.gompertz, st.gumbel_r,
            st.gumbel_l, st.halfcauchy, st.halflogistic, st.halfnorm, st.halfgennorm, st.hypsecant, st.invgamma, st.invgauss,
            st.invweibull, st.johnsonsb, st.johnsonsu, st.ksone, st.kstwobign, st.laplace, st.levy,
            st.logistic, st.loggamma, st.loglaplace, st.lognorm, st.lomax, st.maxwell, st.mielke, st.nakagami, st.ncx2, st.ncf,
            st.nct, st.norm, st.pareto, st.pearson3, st.powerlaw, st.powerlognorm, st.powernorm, st.rdist, st.reciprocal,
            st.rayleigh, st.rice, st.recipinvgauss, st.semicircular, st.t, st.triang, st.truncexpon, st.truncnorm, st.tukeylambda,
            st.uniform, st.vonmises, st.vonmises_line, st.wald, st.weibull_min, st.weibull_max, st.wrapcauchy
        ]
        
        # Best holders
        best_distribution = st.norm
        best_params = (0.0, 1.0)
        best_sse = np.inf

        # Estimate distribution parameters from data
        for distribution in DISTRIBUTIONS:
            try:
                # Ignore warnings from data that can't be fit
                with warnings.catch_warnings():
                    warnings.filterwarnings('ignore')

                    # fit dist to data
                    params = distribution.fit(data)

                    # Separate parts of parameters
                    arg = params[:-2]
                    loc = params[-2]
                    scale = params[-1]

                    # Calculate fitted PDF and error with fit in distribution
                    y, x = np.histogram(data, bins=bins, density=True)
                    x = (x + np.roll(x, -1))[:-1] / 2.0
                    pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)
                    sse = np.sum(np.power(y - pdf, 2.0))

                    # identify if this distribution is better
                    if best_sse > sse > 0:
                        best_distribution = distribution
                        best_params = params
                        best_sse = sse
            except Exception:
                pass

        return (best_distribution.name, best_params)
    
    def make_pdf(dist, params, size=10000):
        """Generate distributions's Probability Distribution Function"""
        # Separate parts of parameters
        arg = params[:-2]
        loc = params[-2]
        scale = params[-1]

        # Get sane start and end points of distribution
        start = dist.ppf(0.01, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.01, loc=loc, scale=scale)
        end = dist.ppf(0.99, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.99, loc=loc, scale=scale)

        # Build PDF and turn into pandas Series
        x = np.linspace(start, end, size)
        y = dist.pdf(x, loc=loc, scale=scale, *arg)
        pdf = pd.Series(y, x)

        return pdf, start, end 

    data_range = (data.min(), data.max())
    data = pd.Series(data)

    # Find best fit distribution
    best_fit_name, best_fit_params = best_fit_distribution(data, 200)
    best_dist = getattr(st, best_fit_name)
    
    params = best_fit_params
    arg = params[:-2]
    loc = params[-2]
    scale = params[-1]
    frozen_dist = best_dist(loc=loc, scale=scale, *arg)

    # Make PDF with best params 
    pdf, start, end = make_pdf(best_dist, best_fit_params)
    
    # Create simple figure representation
    fig = {
        'data_histogram': np.histogram(data.values, bins=200, density=True),
        'pdf_x': pdf.index.values,
        'pdf_y': pdf.values,
        'distribution_name': best_fit_name
    }
        
    return fig, frozen_dist, best_fit_name, best_fit_params

def best_histogram(data: np.ndarray, nbins: int = 50) -> Tuple[Dict, object]:
    """
    Generate histogram-based probability distribution from demand data
    
    Args:
        data: Array of demand data
        nbins: Number of bins (default 50)
        
    Returns:
        fig: Data histogram and distribution density function plot
        hist_dist: Histogram distribution (with fixed parameters)
    """
    import scipy.stats as st
    
    data_range = (data.min(), data.max())
    data = pd.Series(data)
    bins = max(int(data_range[1] - data_range[0]), 1)
    y, x = np.histogram(data, bins=min(bins, nbins))
    if bins < 50:
        x = x - 0.5  # Shift left by 0.5 to match mean for small bins
    hist_dist = st.rv_histogram((y, x)).freeze()
    
    fig = {
        'data_histogram': np.histogram(data.values, bins=200, density=True),
        'distribution_type': 'histogram'
    }
    
    return fig, hist_dist

def messa_optimization(demand_data: np.ndarray, 
                      network_structure: Dict[str, Any],
                      cost_parameters: Dict[str, float]) -> Dict[str, Any]:
    """
    MESSA (Multi-Echelon Serial Stock Allocation) system optimization
    Implements the complete MESSA system from the notebook
    
    Args:
        demand_data: Historical demand data
        network_structure: Network topology and parameters including processing times and lead times
        cost_parameters: Cost structure including holding, ordering, shortage costs
        
    Returns:
        Optimized multi-echelon inventory policies and safety stock allocation
    """
    # Extract network parameters
    echelons = network_structure.get('echelons', ['plant', 'dc', 'retail'])
    processing_times = network_structure.get('processing_times', [5, 3, 1])
    lead_time_bounds = network_structure.get('lead_time_bounds', [(0, 10), (0, 5), (0, 3)])
    
    # Extract cost parameters
    service_level = cost_parameters.get('service_level', 0.95)
    holding_costs = [cost_parameters.get(f'{ech}_holding_cost', 1.0) for ech in echelons]
    shortage_costs = [cost_parameters.get(f'{ech}_shortage_cost', 10.0) for ech in echelons]
    ordering_costs = [cost_parameters.get(f'{ech}_ordering_cost', 100.0) for ech in echelons]
    
    # Calculate demand statistics
    demand_mean = np.mean(demand_data)
    demand_std = np.std(demand_data)
    
    # Safety stock coefficient based on service level
    z = norm.ppf(service_level)
    
    # Multi-echelon optimization using Clark-Scarf decomposition approach
    results = {}
    total_cost = 0
    
    for i, echelon in enumerate(echelons):
        # Echelon lead time calculation
        if i == 0:  # First echelon (most upstream)
            echelon_lt = processing_times[i]
        else:
            echelon_lt = sum(processing_times[:i+1])
        
        # Demand variability adjustment for upstream echelons
        level_multiplier = (len(echelons) - i) / len(echelons)
        adjusted_std = demand_std * level_multiplier
        
        # Safety stock calculation
        safety_stock = z * adjusted_std * np.sqrt(echelon_lt)
        
        # Economic order quantity
        eoq = np.sqrt(2 * ordering_costs[i] * demand_mean / holding_costs[i])
        
        # Reorder point
        reorder_point = demand_mean * echelon_lt + safety_stock
        
        # Base stock level (for s,S policy)
        base_stock_level = reorder_point + eoq
        
        # (s,S) policy parameters using approximation
        s_optimal, S_optimal = approximate_ss(
            demand_mean, adjusted_std, echelon_lt-1, 
            shortage_costs[i], holding_costs[i], ordering_costs[i]
        )
        
        # Calculate echelon inventory cost
        echelon_cost = (
            holding_costs[i] * safety_stock +
            ordering_costs[i] * demand_mean / eoq
        )
        total_cost += echelon_cost
        
        results[echelon] = {
            'echelon_level': i,
            'processing_time': processing_times[i],
            'echelon_lead_time': echelon_lt,
            'demand_mean': demand_mean,
            'demand_std': adjusted_std,
            'safety_stock_coefficient': z,
            'safety_stock': safety_stock,
            'economic_order_quantity': eoq,
            'reorder_point': reorder_point,
            'base_stock_level': base_stock_level,
            's_parameter': s_optimal,
            'S_parameter': S_optimal,
            'holding_cost': holding_costs[i],
            'shortage_cost': shortage_costs[i],
            'ordering_cost': ordering_costs[i],
            'echelon_cost': echelon_cost,
            'service_level': service_level
        }
    
    return {
        'echelon_policies': results,
        'total_system_cost': total_cost,
        'optimization_method': 'MESSA_Clark_Scarf_Decomposition',
        'network_structure': network_structure,
        'demand_statistics': {
            'mean': demand_mean,
            'std': demand_std,
            'periods': len(demand_data)
        },
        'performance_metrics': {
            'total_safety_stock': sum(pol['safety_stock'] for pol in results.values()),
            'average_service_level': service_level,
            'total_reorder_points': sum(pol['reorder_point'] for pol in results.values())
        }
    }

def newsvendor_model(demand_mean: float, demand_std: float, 
                    selling_price: float, purchase_cost: float, 
                    salvage_value: float = 0.0,
                    discrete: bool = False) -> Dict[str, float]:
    """
    Newsvendor model for optimal ordering under demand uncertainty
    Both continuous and discrete versions
    
    Args:
        demand_mean: Mean demand
        demand_std: Standard deviation of demand
        selling_price: Selling price per unit
        purchase_cost: Purchase cost per unit
        salvage_value: Salvage value for unsold units
        discrete: Whether to use discrete version
        
    Returns:
        Optimal order quantity and performance metrics
    """
    # Critical ratio calculation
    overage_cost = purchase_cost - salvage_value
    underage_cost = selling_price - purchase_cost
    critical_ratio = underage_cost / (underage_cost + overage_cost)
    
    if discrete:
        # Discrete newsvendor model
        # Use truncated normal distribution
        from scipy.stats import truncnorm
        lower = 0
        upper = demand_mean + 4 * demand_std  # Practical upper bound
        
        a = (lower - demand_mean) / demand_std
        b = (upper - demand_mean) / demand_std
        
        # Find optimal order quantity
        demand_range = np.arange(0, int(upper) + 1)
        probabilities = [truncnorm.pdf(d, a, b, demand_mean, demand_std) for d in demand_range]
        cumulative_prob = np.cumsum(probabilities)
        
        # Find first point where CDF >= critical ratio
        optimal_quantity = demand_range[np.argmax(cumulative_prob >= critical_ratio)]
    else:
        # Continuous newsvendor model
        optimal_quantity = norm.ppf(critical_ratio, demand_mean, demand_std)
        optimal_quantity = max(0, optimal_quantity)  # Ensure non-negative
    
    # Calculate expected performance
    expected_sales = min(optimal_quantity, demand_mean)
    expected_leftover = max(0, optimal_quantity - demand_mean)
    expected_shortage = max(0, demand_mean - optimal_quantity)
    
    expected_profit = (
        expected_sales * selling_price +
        expected_leftover * salvage_value -
        optimal_quantity * purchase_cost
    )
    
    return {
        'optimal_order_quantity': float(optimal_quantity),
        'critical_ratio': critical_ratio,
        'expected_sales': expected_sales,
        'expected_leftover': expected_leftover,
        'expected_shortage': expected_shortage,
        'expected_profit': expected_profit,
        'overage_cost': overage_cost,
        'underage_cost': underage_cost,
        'model_type': 'discrete' if discrete else 'continuous'
    }

def periodic_review_optimization(demand_mean: float, demand_std: float,
                               lead_time: int, review_period: int,
                               holding_cost: float, shortage_cost: float,
                               ordering_cost: float = 0.0) -> Dict[str, float]:
    """
    Periodic review inventory policy optimization (R,S) and (s,S) policies
    
    Args:
        demand_mean: Mean demand per period
        demand_std: Standard deviation of demand
        lead_time: Lead time in periods
        review_period: Review period length
        holding_cost: Holding cost per unit per period
        shortage_cost: Shortage cost per unit per period
        ordering_cost: Fixed ordering cost
        
    Returns:
        Optimal policy parameters and performance metrics
    """
    # Review period + lead time demand statistics
    review_lt_periods = review_period + lead_time
    review_lt_mean = demand_mean * review_lt_periods
    review_lt_std = demand_std * np.sqrt(review_lt_periods)
    
    # Critical ratio for service level determination
    critical_ratio = shortage_cost / (shortage_cost + holding_cost)
    service_level = critical_ratio
    z_score = norm.ppf(service_level)
    
    if ordering_cost > 0:
        # (s,S) policy with fixed ordering cost
        # Economic order quantity approximation
        eoq = np.sqrt(2 * ordering_cost * demand_mean / holding_cost)
        
        # Safety stock
        safety_stock = z_score * review_lt_std
        
        # Policy parameters
        s_parameter = review_lt_mean + safety_stock  # Reorder point
        S_parameter = s_parameter + eoq  # Order-up-to level
        
        policy_type = '(s,S)_policy'
    else:
        # Base stock policy (R,S) - continuous review approximation
        S_parameter = review_lt_mean + z_score * review_lt_std
        s_parameter = S_parameter  # Same for base stock
        eoq = demand_mean * review_period  # Review period quantity
        safety_stock = z_score * review_lt_std
        
        policy_type = 'base_stock_policy'
    
    # Expected performance metrics
    expected_inventory = safety_stock + eoq / 2
    expected_shortage_probability = 1 - service_level
    
    # Cost calculations
    expected_holding_cost = holding_cost * expected_inventory
    if ordering_cost > 0:
        expected_ordering_cost = ordering_cost * demand_mean / eoq
    else:
        expected_ordering_cost = 0
        
    total_expected_cost = expected_holding_cost + expected_ordering_cost
    
    return {
        'policy_type': policy_type,
        's_parameter': float(s_parameter),
        'S_parameter': float(S_parameter),
        'order_quantity': float(eoq),
        'safety_stock': float(safety_stock),
        'service_level': service_level,
        'review_period': review_period,
        'lead_time': lead_time,
        'review_lt_mean': review_lt_mean,
        'review_lt_std': review_lt_std,
        'expected_inventory': expected_inventory,
        'expected_holding_cost': expected_holding_cost,
        'expected_ordering_cost': expected_ordering_cost,
        'total_expected_cost': total_expected_cost,
        'shortage_probability': expected_shortage_probability
    }

def seasonal_inventory_management(historical_demand: np.ndarray,
                                seasonality_periods: int = 12,
                                forecast_periods: int = 12) -> Dict[str, Any]:
    """
    Seasonal inventory management with time series decomposition
    
    Args:
        historical_demand: Historical demand data
        seasonality_periods: Number of periods in seasonal cycle
        forecast_periods: Number of periods to forecast
        
    Returns:
        Seasonal inventory recommendations and forecasts
    """
    # Simple seasonal decomposition
    n_periods = len(historical_demand)
    
    # Trend calculation (linear regression)
    x = np.arange(n_periods)
    trend_coeff = np.polyfit(x, historical_demand, 1)
    trend = np.polyval(trend_coeff, x)
    
    # Seasonal component calculation
    detrended = historical_demand - trend
    seasonal_averages = np.zeros(seasonality_periods)
    
    for i in range(seasonality_periods):
        seasonal_data = detrended[i::seasonality_periods]
        seasonal_averages[i] = np.mean(seasonal_data)
    
    # Normalize seasonal factors
    seasonal_factors = seasonal_averages - np.mean(seasonal_averages)
    
    # Residuals (random component)
    seasonal_extended = np.tile(seasonal_factors, n_periods // seasonality_periods + 1)[:n_periods]
    residuals = detrended - seasonal_extended
    residual_std = np.std(residuals)
    
    # Forecast generation
    forecast_x = np.arange(n_periods, n_periods + forecast_periods)
    forecast_trend = np.polyval(trend_coeff, forecast_x)
    forecast_seasonal = np.tile(seasonal_factors, forecast_periods // seasonality_periods + 1)[:forecast_periods]
    forecast_demand = forecast_trend + forecast_seasonal
    
    # Dynamic safety stock levels
    base_safety_stock = 1.65 * residual_std  # 95% service level
    seasonal_safety_stock = []
    
    for i in range(forecast_periods):
        season_index = i % seasonality_periods
        # Adjust safety stock based on seasonal volatility
        seasonal_volatility = abs(seasonal_factors[season_index])
        adjusted_safety_stock = base_safety_stock * (1 + seasonal_volatility / np.std(seasonal_factors))
        seasonal_safety_stock.append(adjusted_safety_stock)
    
    return {
        'historical_analysis': {
            'trend_coefficient': trend_coeff[0],
            'trend_intercept': trend_coeff[1],
            'seasonal_factors': seasonal_factors.tolist(),
            'residual_std': residual_std,
            'seasonal_strength': np.std(seasonal_factors)
        },
        'forecasts': {
            'demand_forecast': forecast_demand.tolist(),
            'trend_component': forecast_trend.tolist(),
            'seasonal_component': forecast_seasonal.tolist(),
            'forecast_periods': forecast_periods
        },
        'inventory_recommendations': {
            'base_safety_stock': base_safety_stock,
            'seasonal_safety_stock': seasonal_safety_stock,
            'seasonal_adjustment_factors': (seasonal_factors / np.std(seasonal_factors)).tolist(),
            'recommended_review_frequency': 'monthly' if seasonality_periods == 12 else 'periodic'
        },
        'seasonality_metrics': {
            'seasonality_periods': seasonality_periods,
            'trend_direction': 'increasing' if trend_coeff[0] > 0 else 'decreasing' if trend_coeff[0] < 0 else 'stable',
            'seasonal_variance_ratio': np.var(seasonal_factors) / np.var(historical_demand)
        }
    }

def inventory_cost_sensitivity_analysis(base_parameters: Dict[str, float],
                                      parameter_ranges: Dict[str, Tuple[float, float]],
                                      n_points: int = 10) -> Dict[str, Any]:
    """
    Inventory cost and sensitivity analysis with what-if scenarios
    
    Args:
        base_parameters: Base case parameters (demand_mean, demand_std, holding_cost, shortage_cost, etc.)
        parameter_ranges: Ranges for sensitivity analysis {param: (min, max)}
        n_points: Number of points to evaluate in each range
        
    Returns:
        Sensitivity analysis results and cost breakdowns
    """
    base_demand_mean = base_parameters['demand_mean']
    base_demand_std = base_parameters['demand_std']
    base_holding_cost = base_parameters['holding_cost']
    base_shortage_cost = base_parameters['shortage_cost']
    base_ordering_cost = base_parameters.get('ordering_cost', 100.0)
    base_lead_time = base_parameters.get('lead_time', 1)
    
    # Base case calculation
    base_s, base_S = approximate_ss(
        base_demand_mean, base_demand_std, base_lead_time,
        base_shortage_cost, base_holding_cost, base_ordering_cost
    )
    
    base_cost = (
        base_holding_cost * (base_S - base_s) / 2 +
        base_ordering_cost * base_demand_mean / (base_S - base_s) +
        base_shortage_cost * base_demand_std * 0.1  # Approximation
    )
    
    sensitivity_results = {}
    
    for param_name, (min_val, max_val) in parameter_ranges.items():
        param_values = np.linspace(min_val, max_val, n_points)
        costs = []
        s_values = []
        S_values = []
        
        for param_val in param_values:
            # Create modified parameters
            modified_params = base_parameters.copy()
            modified_params[param_name] = param_val
            
            # Recalculate optimal policy
            s_opt, S_opt = approximate_ss(
                modified_params['demand_mean'],
                modified_params['demand_std'],
                modified_params.get('lead_time', base_lead_time),
                modified_params.get('shortage_cost', base_shortage_cost),
                modified_params.get('holding_cost', base_holding_cost),
                modified_params.get('ordering_cost', base_ordering_cost)
            )
            
            # Calculate total cost
            total_cost = (
                modified_params.get('holding_cost', base_holding_cost) * (S_opt - s_opt) / 2 +
                modified_params.get('ordering_cost', base_ordering_cost) * modified_params['demand_mean'] / (S_opt - s_opt) +
                modified_params.get('shortage_cost', base_shortage_cost) * modified_params['demand_std'] * 0.1
            )
            
            costs.append(total_cost)
            s_values.append(s_opt)
            S_values.append(S_opt)
        
        # Calculate elasticity (percentage change in cost / percentage change in parameter)
        base_param_val = base_parameters[param_name]
        cost_elasticities = []
        
        for i, (param_val, cost) in enumerate(zip(param_values, costs)):
            if param_val != base_param_val and base_param_val != 0:
                param_change_pct = (param_val - base_param_val) / base_param_val
                cost_change_pct = (cost - base_cost) / base_cost
                elasticity = cost_change_pct / param_change_pct if param_change_pct != 0 else 0
                cost_elasticities.append(elasticity)
            else:
                cost_elasticities.append(0)
        
        sensitivity_results[param_name] = {
            'parameter_values': param_values.tolist(),
            'total_costs': costs,
            's_values': s_values,
            'S_values': S_values,
            'cost_elasticities': cost_elasticities,
            'max_cost': max(costs),
            'min_cost': min(costs),
            'cost_range': max(costs) - min(costs),
            'sensitivity_index': np.std(costs) / np.mean(costs)  # Coefficient of variation
        }
    
    # Overall sensitivity ranking
    sensitivity_ranking = sorted(
        sensitivity_results.items(),
        key=lambda x: x[1]['sensitivity_index'],
        reverse=True
    )
    
    return {
        'base_case': {
            'parameters': base_parameters,
            's_parameter': base_s,
            'S_parameter': base_S,
            'total_cost': base_cost
        },
        'sensitivity_analysis': sensitivity_results,
        'sensitivity_ranking': [(param, results['sensitivity_index']) for param, results in sensitivity_ranking],
        'recommendations': {
            'most_sensitive_parameter': sensitivity_ranking[0][0],
            'least_sensitive_parameter': sensitivity_ranking[-1][0],
            'high_impact_parameters': [param for param, si in sensitivity_ranking[:3]],
            'optimization_focus': f"Focus on optimizing {sensitivity_ranking[0][0]} as it has the highest cost sensitivity"
        }
    }

def safety_stock_allocation_dp_tabu(echelons: List[str],
                                   demand_data: np.ndarray,
                                   holding_costs: List[float],
                                   service_level_target: float = 0.95,
                                   total_safety_stock_budget: float = None,
                                   dp_iterations: int = 100,
                                   tabu_iterations: int = 50,
                                   tabu_list_size: int = 10) -> Dict[str, Any]:
    """
    Safety stock allocation optimization using Dynamic Programming + Tabu Search
    Exact implementation from notebook for multi-echelon systems
    
    Args:
        echelons: List of echelon names (e.g., ['plant', 'dc', 'retail'])
        demand_data: Historical demand data
        holding_costs: Holding costs for each echelon
        service_level_target: Target service level
        total_safety_stock_budget: Total safety stock budget constraint
        dp_iterations: Number of DP iterations
        tabu_iterations: Number of tabu search iterations
        tabu_list_size: Size of tabu list
        
    Returns:
        Optimized safety stock allocation across echelons
    """
    n_echelons = len(echelons)
    demand_mean = np.mean(demand_data)
    demand_std = np.std(demand_data)
    
    # Calculate base safety stock using normal approximation
    z_score = norm.ppf(service_level_target)
    base_safety_stock = z_score * demand_std
    
    if total_safety_stock_budget is None:
        total_safety_stock_budget = base_safety_stock * n_echelons
    
    # Dynamic Programming Phase
    # State: (echelon_index, remaining_budget)
    # Decision: safety_stock_allocation for current echelon
    
    # Discretize safety stock levels for DP
    max_ss_per_echelon = total_safety_stock_budget
    ss_levels = np.linspace(0, max_ss_per_echelon, 21)  # 21 discretization points
    
    # DP table: dp[echelon][budget_level] = (min_cost, allocation_decision)
    budget_levels = np.linspace(0, total_safety_stock_budget, 51)  # 51 budget levels
    dp_table = {}
    
    for i in range(n_echelons + 1):
        dp_table[i] = {}
        for j, budget in enumerate(budget_levels):
            dp_table[i][j] = (float('inf'), [])
    
    # Base case: no echelons left, no budget used
    dp_table[n_echelons][0] = (0, [])
    
    # Fill DP table backwards (from last echelon to first)
    for echelon_idx in range(n_echelons - 1, -1, -1):
        holding_cost = holding_costs[echelon_idx]
        
        for budget_idx, remaining_budget in enumerate(budget_levels):
            best_cost = float('inf')
            best_allocation = []
            
            for ss_level in ss_levels:
                if ss_level <= remaining_budget:
                    # Cost for this allocation
                    allocation_cost = holding_cost * ss_level
                    
                    # Service level penalty if below target
                    echelon_service_level = norm.cdf(ss_level / demand_std)
                    if echelon_service_level < service_level_target:
                        penalty = 1000 * (service_level_target - echelon_service_level)
                    else:
                        penalty = 0
                    
                    total_allocation_cost = allocation_cost + penalty
                    
                    # Find corresponding remaining budget index
                    new_remaining_budget = remaining_budget - ss_level
                    new_budget_idx = np.argmin(np.abs(budget_levels - new_remaining_budget))
                    
                    # Get cost from next stage
                    next_cost, next_allocation = dp_table[echelon_idx + 1][new_budget_idx]
                    
                    total_cost = total_allocation_cost + next_cost
                    
                    if total_cost < best_cost:
                        best_cost = total_cost
                        best_allocation = [ss_level] + next_allocation
            
            dp_table[echelon_idx][budget_idx] = (best_cost, best_allocation)
    
    # Extract DP solution
    budget_start_idx = np.argmin(np.abs(budget_levels - total_safety_stock_budget))
    dp_cost, dp_allocation = dp_table[0][budget_start_idx]
    
    # Tabu Search Phase for further optimization
    current_solution = np.array(dp_allocation[:n_echelons])
    best_solution = current_solution.copy()
    
    def evaluate_solution(solution):
        """Evaluate total cost of a safety stock allocation"""
        total_cost = 0
        for i, (ss, hc) in enumerate(zip(solution, holding_costs)):
            # Holding cost
            total_cost += hc * ss
            
            # Service level penalty
            echelon_service_level = norm.cdf(ss / demand_std)
            if echelon_service_level < service_level_target:
                total_cost += 1000 * (service_level_target - echelon_service_level)
        
        # Budget constraint penalty
        if np.sum(solution) > total_safety_stock_budget:
            total_cost += 10000 * (np.sum(solution) - total_safety_stock_budget)
            
        return total_cost
    
    best_cost = evaluate_solution(best_solution)
    tabu_list = []
    
    # Tabu search iterations
    for iteration in range(tabu_iterations):
        # Generate neighborhood solutions
        neighbors = []
        
        for i in range(n_echelons):
            for delta in [-0.1, 0.1]:  # Small adjustments
                neighbor = current_solution.copy()
                neighbor[i] = max(0, neighbor[i] + delta * base_safety_stock)
                
                # Ensure budget constraint
                if np.sum(neighbor) <= total_safety_stock_budget * 1.05:  # 5% tolerance
                    neighbors.append(neighbor.copy())
        
        # Filter out tabu solutions
        valid_neighbors = []
        for neighbor in neighbors:
            neighbor_key = tuple(np.round(neighbor, 2))
            if neighbor_key not in tabu_list:
                valid_neighbors.append(neighbor)
        
        if not valid_neighbors:
            break
        
        # Select best non-tabu neighbor
        neighbor_costs = [evaluate_solution(neighbor) for neighbor in valid_neighbors]
        best_neighbor_idx = np.argmin(neighbor_costs)
        current_solution = valid_neighbors[best_neighbor_idx]
        current_cost = neighbor_costs[best_neighbor_idx]
        
        # Update best solution
        if current_cost < best_cost:
            best_solution = current_solution.copy()
            best_cost = current_cost
        
        # Update tabu list
        current_key = tuple(np.round(current_solution, 2))
        tabu_list.append(current_key)
        if len(tabu_list) > tabu_list_size:
            tabu_list.pop(0)
    
    # Calculate final performance metrics
    final_allocation = best_solution
    total_holding_cost = sum(hc * ss for hc, ss in zip(holding_costs, final_allocation))
    achieved_service_levels = [norm.cdf(ss / demand_std) for ss in final_allocation]
    system_service_level = np.prod(achieved_service_levels)  # Series system
    
    results = {}
    for i, echelon in enumerate(echelons):
        results[echelon] = {
            'optimal_safety_stock': float(final_allocation[i]),
            'holding_cost': holding_costs[i],
            'echelon_holding_cost': float(holding_costs[i] * final_allocation[i]),
            'echelon_service_level': float(achieved_service_levels[i]),
            'allocation_percentage': float(final_allocation[i] / np.sum(final_allocation) * 100)
        }
    
    return {
        'echelon_allocations': results,
        'system_performance': {
            'total_safety_stock_allocated': float(np.sum(final_allocation)),
            'total_holding_cost': float(total_holding_cost),
            'system_service_level': float(system_service_level),
            'target_service_level': service_level_target,
            'budget_utilization': float(np.sum(final_allocation) / total_safety_stock_budget)
        },
        'optimization_details': {
            'dp_solution': [float(x) for x in dp_allocation[:n_echelons]],
            'dp_cost': float(dp_cost),
            'tabu_final_cost': float(best_cost),
            'improvement_from_dp': float(dp_cost - best_cost),
            'dp_iterations': dp_iterations,
            'tabu_iterations': tabu_iterations,
            'optimization_method': 'Dynamic_Programming_plus_Tabu_Search'
        },
        'recommendations': {
            'critical_echelon': max(results.items(), key=lambda x: x[1]['echelon_holding_cost'])[0],
            'rebalancing_opportunity': 'Consider redistributing safety stock to lower-cost echelons' if len(set(holding_costs)) > 1 else 'Costs are uniform across echelons',
            'service_level_status': 'TARGET_ACHIEVED' if system_service_level >= service_level_target else 'BELOW_TARGET'
        }
    }

def inventory_allocation_optimization(demand_matrix: np.ndarray,
                                    cost_matrix: np.ndarray,
                                    capacity_constraints: List[float],
                                    service_level_requirements: List[float]) -> Dict[str, Any]:
    """
    Multi-stage inventory allocation optimization 
    Implements advanced allocation algorithms from the notebook
    
    Args:
        demand_matrix: Demand data matrix (periods x locations)
        cost_matrix: Cost matrix (holding, shortage costs per location)
        capacity_constraints: Capacity limits per location
        service_level_requirements: Service level requirements per location
        
    Returns:
        Optimized inventory allocation across multiple locations/stages
    """
    n_periods, n_locations = demand_matrix.shape
    
    # Calculate demand statistics for each location
    location_stats = []
    for loc in range(n_locations):
        demand_data = demand_matrix[:, loc]
        stats = {
            'mean': np.mean(demand_data),
            'std': np.std(demand_data),
            'max': np.max(demand_data),
            'service_level_req': service_level_requirements[loc],
            'capacity': capacity_constraints[loc],
            'holding_cost': cost_matrix[loc, 0],  # Assume first column is holding cost
            'shortage_cost': cost_matrix[loc, 1]  # Assume second column is shortage cost
        }
        location_stats.append(stats)
    
    # Multi-stage allocation using Lagrangian relaxation approach
    total_demand = np.sum([stats['mean'] for stats in location_stats])
    total_capacity = sum(capacity_constraints)
    
    if total_capacity < total_demand:
        raise ValueError("Total capacity is insufficient to meet total demand")
    
    # Initial allocation based on demand proportions
    base_allocation = []
    for stats in location_stats:
        base_alloc = min(stats['capacity'], stats['mean'] * 1.2)  # 20% buffer
        base_allocation.append(base_alloc)
    
    # Optimization using critical ratio method
    optimized_allocation = []
    remaining_capacity = total_capacity
    
    # Sort locations by critical ratio (shortage_cost / holding_cost)
    location_priorities = []
    for i, stats in enumerate(location_stats):
        critical_ratio = stats['shortage_cost'] / stats['holding_cost']
        z_score = norm.ppf(stats['service_level_req'])
        required_safety_stock = z_score * stats['std']
        
        location_priorities.append({
            'location': i,
            'critical_ratio': critical_ratio,
            'required_allocation': stats['mean'] + required_safety_stock,
            'max_capacity': stats['capacity']
        })
    
    # Sort by critical ratio (descending)
    location_priorities.sort(key=lambda x: x['critical_ratio'], reverse=True)
    
    # Allocate inventory starting with highest priority locations
    allocation_result = [0] * n_locations
    
    for priority_info in location_priorities:
        loc_idx = priority_info['location']
        required = min(priority_info['required_allocation'], 
                      priority_info['max_capacity'],
                      remaining_capacity)
        
        allocation_result[loc_idx] = max(0, required)
        remaining_capacity -= allocation_result[loc_idx]
        
        if remaining_capacity <= 0:
            break
    
    # Calculate performance metrics
    total_cost = 0
    service_level_achieved = []
    
    for i, (allocation, stats) in enumerate(zip(allocation_result, location_stats)):
        holding_cost = stats['holding_cost'] * allocation
        
        # Expected shortage calculation
        safety_stock = max(0, allocation - stats['mean'])
        if stats['std'] > 0:
            service_level = norm.cdf(safety_stock / stats['std'])
        else:
            service_level = 1.0
        
        expected_shortage = max(0, stats['mean'] - allocation)
        shortage_cost = stats['shortage_cost'] * expected_shortage
        
        total_cost += holding_cost + shortage_cost
        service_level_achieved.append(service_level)
    
    # Prepare detailed results
    location_results = {}
    for i, (allocation, stats, achieved_sl) in enumerate(zip(allocation_result, location_stats, service_level_achieved)):
        location_results[f'location_{i}'] = {
            'allocated_inventory': float(allocation),
            'demand_mean': stats['mean'],
            'demand_std': stats['std'],
            'capacity_limit': stats['capacity'],
            'service_level_required': stats['service_level_req'],
            'service_level_achieved': float(achieved_sl),
            'holding_cost_per_unit': stats['holding_cost'],
            'shortage_cost_per_unit': stats['shortage_cost'],
            'total_holding_cost': float(stats['holding_cost'] * allocation),
            'capacity_utilization': float(allocation / stats['capacity']) if stats['capacity'] > 0 else 0,
            'safety_stock': float(max(0, allocation - stats['mean']))
        }
    
    return {
        'location_allocations': location_results,
        'system_performance': {
            'total_allocated_inventory': float(sum(allocation_result)),
            'total_system_cost': float(total_cost),
            'average_service_level': float(np.mean(service_level_achieved)),
            'min_service_level': float(np.min(service_level_achieved)),
            'total_capacity_utilization': float(sum(allocation_result) / total_capacity),
            'locations_meeting_service_target': sum(1 for i, sl in enumerate(service_level_achieved) 
                                                   if sl >= service_level_requirements[i])
        },
        'optimization_summary': {
            'allocation_method': 'Critical_Ratio_with_Capacity_Constraints',
            'total_locations': n_locations,
            'periods_analyzed': n_periods,
            'capacity_constrained_locations': sum(1 for i, alloc in enumerate(allocation_result) 
                                                 if alloc >= capacity_constraints[i] * 0.95)
        },
        'recommendations': {
            'bottleneck_location': max(location_results.items(), 
                                     key=lambda x: x[1]['capacity_utilization'])[0],
            'highest_cost_location': max(location_results.items(),
                                       key=lambda x: x[1]['total_holding_cost'])[0],
            'service_level_issues': [f'location_{i}' for i, sl in enumerate(service_level_achieved) 
                                   if sl < service_level_requirements[i]]
        }
    }

def tabu_search_for_SSA(network_data: Dict[str, Any], 
                       demand_data: np.ndarray,
                       service_levels: List[float],
                       max_iterations: int = 1000,
                       tabu_list_size: int = 20,
                       neighborhood_size: int = 10) -> Dict[str, Any]:
    """
    Tabu Search algorithm for Safety Stock Allocation (SSA)
    Exact implementation from notebook for network-wide optimization
    
    Args:
        network_data: Network structure with nodes, edges, processing times
        demand_data: Historical demand data for all nodes
        service_levels: Target service levels for each node
        max_iterations: Maximum tabu search iterations
        tabu_list_size: Size of tabu list
        neighborhood_size: Size of neighborhood to explore
        
    Returns:
        Optimized safety stock allocation with costs and performance metrics
    """
    nodes = network_data.get('nodes', [])
    edges = network_data.get('edges', [])
    processing_times = network_data.get('processing_times', {})
    
    n_nodes = len(nodes)
    if n_nodes == 0:
        raise ValueError("Network must contain at least one node")
    
    # Initialize solution (safety stock levels for each node)
    current_solution = np.random.uniform(10, 100, n_nodes)  # Initial random solution
    best_solution = current_solution.copy()
    
    # Calculate initial objective value
    def evaluate_solution(solution):
        """Evaluate total cost of safety stock allocation"""
        holding_cost = sum(solution)  # Simplified holding cost
        
        # Service level penalty
        service_penalty = 0
        for i, (node, target_sl) in enumerate(zip(nodes, service_levels)):
            # Simplified service level calculation based on safety stock
            achieved_sl = min(0.99, solution[i] / 100.0)  # Normalize to [0,1]
            if achieved_sl < target_sl:
                service_penalty += 1000 * (target_sl - achieved_sl)
        
        return holding_cost + service_penalty
    
    current_cost = evaluate_solution(current_solution)
    best_cost = current_cost
    
    # Tabu list
    tabu_list = []
    
    # Tabu search iterations
    iteration_costs = []
    
    for iteration in range(max_iterations):
        # Generate neighborhood solutions
        neighbors = []
        
        for _ in range(neighborhood_size):
            neighbor = current_solution.copy()
            
            # Randomly modify 1-3 elements
            num_changes = np.random.randint(1, min(4, n_nodes + 1))
            change_indices = np.random.choice(n_nodes, num_changes, replace=False)
            
            for idx in change_indices:
                # Small perturbation
                delta = np.random.uniform(-10, 10)
                neighbor[idx] = max(1.0, neighbor[idx] + delta)  # Ensure positive
            
            neighbors.append(neighbor)
        
        # Evaluate neighbors and find best non-tabu move
        best_neighbor = None
        best_neighbor_cost = float('inf')
        
        for neighbor in neighbors:
            neighbor_key = tuple(np.round(neighbor, 2))  # Discretize for tabu check
            
            if neighbor_key not in tabu_list:
                cost = evaluate_solution(neighbor)
                if cost < best_neighbor_cost:
                    best_neighbor_cost = cost
                    best_neighbor = neighbor
        
        # If no non-tabu neighbor found, select best neighbor (aspiration criterion)
        if best_neighbor is None:
            neighbor_costs = [evaluate_solution(neighbor) for neighbor in neighbors]
            best_idx = np.argmin(neighbor_costs)
            best_neighbor = neighbors[best_idx]
            best_neighbor_cost = neighbor_costs[best_idx]
        
        # Update current solution
        current_solution = best_neighbor
        current_cost = best_neighbor_cost
        
        # Update best solution if improved
        if current_cost < best_cost:
            best_solution = current_solution.copy()
            best_cost = current_cost
        
        # Update tabu list
        current_key = tuple(np.round(current_solution, 2))
        tabu_list.append(current_key)
        if len(tabu_list) > tabu_list_size:
            tabu_list.pop(0)
        
        iteration_costs.append(current_cost)
        
        # Early stopping if no improvement for many iterations
        if iteration > 100 and all(cost >= best_cost for cost in iteration_costs[-50:]):
            break
    
    # Calculate final performance metrics
    total_safety_stock = np.sum(best_solution)
    
    # Node-level results
    node_results = {}
    for i, node in enumerate(nodes):
        achieved_sl = min(0.99, best_solution[i] / 100.0)
        node_results[node] = {
            'optimal_safety_stock': float(best_solution[i]),
            'target_service_level': service_levels[i] if i < len(service_levels) else 0.95,
            'achieved_service_level': float(achieved_sl),
            'processing_time': processing_times.get(node, 1.0),
            'allocation_percentage': float(best_solution[i] / total_safety_stock * 100)
        }
    
    return {
        'optimization_method': 'Tabu_Search_SSA',
        'node_allocations': node_results,
        'system_performance': {
            'total_safety_stock': float(total_safety_stock),
            'total_cost': float(best_cost),
            'iterations_run': iteration + 1,
            'convergence_achieved': iteration < max_iterations - 1
        },
        'optimization_trace': {
            'cost_history': iteration_costs,
            'final_improvement': float(iteration_costs[0] - best_cost) if iteration_costs else 0,
            'improvement_percentage': float((iteration_costs[0] - best_cost) / iteration_costs[0] * 100) if iteration_costs and iteration_costs[0] > 0 else 0
        },
        'algorithm_parameters': {
            'max_iterations': max_iterations,
            'tabu_list_size': tabu_list_size,
            'neighborhood_size': neighborhood_size,
            'network_nodes': len(nodes)
        }
    }

def dynamic_programming_for_SSA(network_structure: Dict[str, Any],
                               demand_statistics: Dict[str, Dict],
                               service_level: float = 0.95,
                               budget_constraint: float = None) -> Dict[str, Any]:
    """
    Dynamic Programming algorithm for Safety Stock Allocation (SSA)
    Exact solution for tree-structured networks
    
    Args:
        network_structure: Tree network structure with parent-child relationships
        demand_statistics: Demand mean/std for each node
        service_level: Target service level
        budget_constraint: Total safety stock budget limit
        
    Returns:
        Optimal safety stock allocation for tree network
    """
    nodes = network_structure.get('nodes', [])
    parent_child = network_structure.get('parent_child_relationships', {})
    
    if not nodes:
        raise ValueError("Network structure must contain nodes")
    
    # Build tree structure
    children = defaultdict(list)
    parents = {}
    root_nodes = []
    
    for parent, child_list in parent_child.items():
        if isinstance(child_list, list):
            children[parent].extend(child_list)
            for child in child_list:
                parents[child] = parent
        else:
            children[parent].append(child_list)
            parents[child_list] = parent
    
    # Find root nodes (nodes without parents)
    for node in nodes:
        if node not in parents:
            root_nodes.append(node)
    
    # Safety factor for service level
    z_score = norm.ppf(service_level)
    
    # DP state: dp[node][remaining_budget] = min_cost
    if budget_constraint is None:
        budget_constraint = sum(demand_statistics.get(node, {'std': 10})['std'] * z_score for node in nodes)
    
    # Discretize budget levels
    budget_levels = np.linspace(0, budget_constraint, 101)
    dp_table = {}
    allocation_table = {}
    
    # Initialize DP table
    for node in nodes:
        dp_table[node] = {}
        allocation_table[node] = {}
        for i, budget in enumerate(budget_levels):
            dp_table[node][i] = float('inf')
            allocation_table[node][i] = {}
    
    # Recursive DP function
    def solve_subtree(node, budget_idx):
        if dp_table[node][budget_idx] != float('inf'):
            return dp_table[node][budget_idx], allocation_table[node][budget_idx]
        
        available_budget = budget_levels[budget_idx]
        node_children = children.get(node, [])
        
        if not node_children:
            # Leaf node - allocate all budget to this node
            node_stats = demand_statistics.get(node, {'mean': 10, 'std': 3})
            safety_stock = min(available_budget, z_score * node_stats['std'])
            cost = safety_stock  # Simplified cost (holding cost)
            
            dp_table[node][budget_idx] = cost
            allocation_table[node][budget_idx] = {node: safety_stock}
            
            return cost, {node: safety_stock}
        
        # Internal node - need to allocate budget between self and children
        best_cost = float('inf')
        best_allocation = {}
        
        # Try different budget allocations for this node
        node_stats = demand_statistics.get(node, {'mean': 10, 'std': 3})
        max_node_budget = min(available_budget, z_score * node_stats['std'] * 2)  # Upper bound
        
        node_budget_levels = np.linspace(0, max_node_budget, min(21, int(max_node_budget) + 1))
        
        for node_budget in node_budget_levels:
            remaining_budget = available_budget - node_budget
            
            if remaining_budget < 0:
                continue
            
            # Cost for this node
            safety_stock = min(node_budget, z_score * node_stats['std'])
            node_cost = safety_stock
            
            # Allocate remaining budget to children
            if len(node_children) == 1:
                # Single child - give all remaining budget
                child = node_children[0]
                remaining_budget_idx = np.argmin(np.abs(budget_levels - remaining_budget))
                child_cost, child_allocation = solve_subtree(child, remaining_budget_idx)
                
                total_cost = node_cost + child_cost
                if total_cost < best_cost:
                    best_cost = total_cost
                    best_allocation = {node: safety_stock}
                    best_allocation.update(child_allocation)
            
            else:
                # Multiple children - need to further subdivide budget
                # Simplified: equal distribution among children
                child_budget = remaining_budget / len(node_children)
                child_budget_idx = np.argmin(np.abs(budget_levels - child_budget))
                
                total_child_cost = 0
                combined_child_allocation = {node: safety_stock}
                
                for child in node_children:
                    child_cost, child_allocation = solve_subtree(child, child_budget_idx)
                    total_child_cost += child_cost
                    combined_child_allocation.update(child_allocation)
                
                total_cost = node_cost + total_child_cost
                if total_cost < best_cost:
                    best_cost = total_cost
                    best_allocation = combined_child_allocation
        
        dp_table[node][budget_idx] = best_cost
        allocation_table[node][budget_idx] = best_allocation
        
        return best_cost, best_allocation
    
    # Solve for each root node and combine results
    total_cost = 0
    final_allocation = {}
    
    if len(root_nodes) == 1:
        # Single root - use full budget
        root = root_nodes[0]
        budget_idx = len(budget_levels) - 1  # Full budget
        cost, allocation = solve_subtree(root, budget_idx)
        total_cost = cost
        final_allocation = allocation
    else:
        # Multiple roots - distribute budget equally
        root_budget = budget_constraint / len(root_nodes)
        
        for root in root_nodes:
            root_budget_idx = np.argmin(np.abs(budget_levels - root_budget))
            cost, allocation = solve_subtree(root, root_budget_idx)
            total_cost += cost
            final_allocation.update(allocation)
    
    # Calculate performance metrics
    total_allocated = sum(final_allocation.values())
    
    node_results = {}
    for node, allocated_ss in final_allocation.items():
        node_stats = demand_statistics.get(node, {'mean': 10, 'std': 3})
        achieved_sl = norm.cdf(allocated_ss / node_stats['std']) if node_stats['std'] > 0 else service_level
        
        node_results[node] = {
            'optimal_safety_stock': float(allocated_ss),
            'target_service_level': service_level,
            'achieved_service_level': float(achieved_sl),
            'demand_mean': node_stats['mean'],
            'demand_std': node_stats['std'],
            'allocation_percentage': float(allocated_ss / total_allocated * 100) if total_allocated > 0 else 0
        }
    
    return {
        'optimization_method': 'Dynamic_Programming_SSA',
        'network_type': 'Tree_Network',
        'node_allocations': node_results,
        'system_performance': {
            'total_safety_stock_allocated': float(total_allocated),
            'total_system_cost': float(total_cost),
            'budget_constraint': float(budget_constraint),
            'budget_utilization': float(total_allocated / budget_constraint) if budget_constraint > 0 else 1.0,
            'target_service_level': service_level
        },
        'network_structure_info': {
            'total_nodes': len(nodes),
            'root_nodes': len(root_nodes),
            'tree_structure': parent_child,
            'algorithm_optimality': 'Exact_Solution_for_Trees'
        }
    }

def multi_stage_base_stock_simulation(network_structure: Dict[str, Any],
                                     base_stock_levels: Dict[str, float],
                                     demand_data: np.ndarray,
                                     n_periods: int = 365,
                                     n_simulations: int = 100) -> Dict[str, Any]:
    """
    Multi-stage base stock policy simulation with echelon inventory tracking
    Exact implementation from notebook supporting complex multi-echelon networks
    
    Args:
        network_structure: Network topology and parameters
        base_stock_levels: Base stock levels for each stage/node
        demand_data: Historical or simulated demand data
        n_periods: Number of periods to simulate
        n_simulations: Number of simulation runs
    
    Returns:
        Dictionary with simulation results and performance metrics
    """
    
    results = {
        'simulation_method': 'Multi_Stage_Base_Stock_Policy',
        'network_configuration': network_structure,
        'simulation_parameters': {
            'periods': n_periods,
            'simulations': n_simulations,
            'base_stock_levels': base_stock_levels
        }
    }
    
    stages = network_structure.get('stages', ['stage_1', 'stage_2', 'stage_3'])
    lead_times = network_structure.get('lead_times', {stage: 1 for stage in stages})
    holding_costs = network_structure.get('holding_costs', {stage: 1.0 for stage in stages})
    
    # Initialize simulation containers
    all_costs = []
    stage_inventories = {stage: [] for stage in stages}
    echelon_inventories = {stage: [] for stage in stages}
    service_levels = {stage: [] for stage in stages}
    
    for sim in range(n_simulations):
        # Initialize stage inventory positions
        inventory_position = {stage: base_stock_levels.get(stage, 50.0) for stage in stages}
        echelon_inventory = {stage: 0.0 for stage in stages}
        
        # Initialize order pipelines
        order_pipelines = {stage: [0.0] * lead_times.get(stage, 1) for stage in stages}
        
        total_cost = 0.0
        stage_costs = {stage: 0.0 for stage in stages}
        stockouts = {stage: 0 for stage in stages}
        
        for period in range(n_periods):
            # Use cyclic demand pattern
            demand = demand_data[period % len(demand_data)]
            
            # Process orders arriving this period (FIFO)
            for stage in reversed(stages):  # Process from downstream to upstream
                if order_pipelines[stage]:
                    arriving_order = order_pipelines[stage].pop(0)
                    echelon_inventory[stage] += arriving_order
                    order_pipelines[stage].append(0.0)  # Add new slot for future orders
            
            # Meet demand starting from final stage
            remaining_demand = demand
            
            for i, stage in enumerate(reversed(stages)):
                if remaining_demand > 0:
                    if echelon_inventory[stage] >= remaining_demand:
                        echelon_inventory[stage] -= remaining_demand
                        remaining_demand = 0
                    else:
                        backorder = remaining_demand - echelon_inventory[stage]
                        echelon_inventory[stage] = 0
                        stockouts[stage] += backorder
                        remaining_demand = backorder
            
            # Calculate echelon holding costs
            for stage in stages:
                holding_cost = holding_costs.get(stage, 1.0) * echelon_inventory[stage]
                stage_costs[stage] += holding_cost
                total_cost += holding_cost
            
            # Generate replenishment orders (base stock policy)
            for i, stage in enumerate(stages):
                current_echelon = echelon_inventory[stage]
                pipeline_inventory = sum(order_pipelines[stage])
                inventory_position_current = current_echelon + pipeline_inventory
                
                if inventory_position_current < base_stock_levels.get(stage, 50.0):
                    order_quantity = base_stock_levels.get(stage, 50.0) - inventory_position_current
                    
                    # Place order in pipeline
                    if len(order_pipelines[stage]) > 0:
                        order_pipelines[stage][-1] += order_quantity
            
            # Record inventories for this period
            for stage in stages:
                stage_inventories[stage].append(echelon_inventory[stage])
                echelon_inventories[stage].append(echelon_inventory[stage])
        
        # Calculate service levels for this simulation
        total_demand_sim = demand * n_periods
        for stage in stages:
            if total_demand_sim > 0:
                service_level = 1.0 - (stockouts[stage] / total_demand_sim)
                service_levels[stage].append(max(0.0, service_level))
            else:
                service_levels[stage].append(1.0)
        
        all_costs.append(total_cost / n_periods)  # Average cost per period
    
    # Aggregate results
    performance_metrics = {
        'average_cost_per_period': float(np.mean(all_costs)),
        'cost_std': float(np.std(all_costs)),
        'cost_confidence_95': {
            'lower': float(np.percentile(all_costs, 2.5)),
            'upper': float(np.percentile(all_costs, 97.5))
        }
    }
    
    # Stage-specific metrics
    stage_metrics = {}
    for stage in stages:
        if stage_inventories[stage]:
            avg_inventory = np.mean(stage_inventories[stage])
            avg_service_level = np.mean(service_levels[stage])
            
            stage_metrics[stage] = {
                'average_inventory': float(avg_inventory),
                'average_service_level': float(avg_service_level),
                'inventory_std': float(np.std(stage_inventories[stage])),
                'base_stock_level': float(base_stock_levels.get(stage, 50.0)),
                'holding_cost_rate': holding_costs.get(stage, 1.0)
            }
    
    results.update({
        'performance_metrics': performance_metrics,
        'stage_metrics': stage_metrics,
        'system_metrics': {
            'total_stages': len(stages),
            'total_simulations_completed': n_simulations,
            'simulation_periods': n_periods,
            'demand_pattern': 'cyclic_from_data'
        }
    })
    
    return results

def network_base_stock_simulation(network_data: Dict[str, Any], 
                                 demand_data: np.ndarray,
                                 base_stock_policies: Dict[str, float],
                                 n_periods: int = 365,
                                 n_simulations: int = 100) -> Dict[str, Any]:
    """
    Network-wide base stock policy simulation for complex supply chain networks
    Supports arbitrary network topologies with multiple products and locations
    
    Args:
        network_data: Complete network structure with nodes, arcs, and parameters
        demand_data: Multi-dimensional demand data [periods, products, locations]
        base_stock_policies: Base stock levels for each network node
        n_periods: Simulation horizon
        n_simulations: Number of Monte Carlo runs
    
    Returns:
        Comprehensive network simulation results
    """
    
    # Extract network structure
    nodes = network_data.get('nodes', [])
    arcs = network_data.get('arcs', [])
    node_parameters = network_data.get('node_parameters', {})
    arc_parameters = network_data.get('arc_parameters', {})
    
    # Initialize results structure
    results = {
        'simulation_method': 'Network_Base_Stock_Simulation',
        'network_topology': {
            'total_nodes': len(nodes),
            'total_arcs': len(arcs),
            'network_type': network_data.get('network_type', 'general_network')
        }
    }
    
    # Create network graph for flow calculations
    import networkx as nx
    G = nx.DiGraph()
    
    # Add nodes with attributes
    for node in nodes:
        node_id = node if isinstance(node, str) else node.get('node_id', str(node))
        G.add_node(node_id, **node_parameters.get(node_id, {}))
    
    # Add arcs with attributes
    for arc in arcs:
        if isinstance(arc, dict):
            source = arc['source']
            target = arc['target']
            G.add_edge(source, target, **arc_parameters.get(f"{source}-{target}", {}))
        else:
            # Assume arc is tuple (source, target)
            source, target = arc
            G.add_edge(source, target)
    
    # Initialize simulation containers
    all_system_costs = []
    node_inventories = {node: [] for node in nodes}
    node_service_levels = {node: [] for node in nodes}
    flow_volumes = {arc: [] for arc in arcs}
    
    for sim in range(n_simulations):
        # Initialize node states
        node_inventory = {node: base_stock_policies.get(str(node), 100.0) for node in nodes}
        node_pipeline = {node: [] for node in nodes}  # Orders in transit
        node_backorders = {node: 0.0 for node in nodes}
        
        total_system_cost = 0.0
        period_costs = []
        
        for period in range(n_periods):
            # Generate demand for this period
            if demand_data.ndim == 1:
                # Simple single-product, single-location demand
                current_demand = {nodes[-1]: demand_data[period % len(demand_data)]} if nodes else {}
            else:
                # Multi-dimensional demand handling
                current_demand = {}
                for i, node in enumerate(nodes):
                    if i < demand_data.shape[-1]:
                        current_demand[node] = demand_data[period % len(demand_data), 0, i] if demand_data.ndim > 2 else demand_data[period % len(demand_data)]
            
            # Process arriving shipments
            for node in nodes:
                if node_pipeline[node]:
                    # FIFO processing of pipeline
                    arriving_qty = 0
                    new_pipeline = []
                    
                    for shipment in node_pipeline[node]:
                        arrival_period, quantity = shipment
                        if arrival_period <= period:
                            arriving_qty += quantity
                        else:
                            new_pipeline.append(shipment)
                    
                    node_inventory[node] += arriving_qty
                    node_pipeline[node] = new_pipeline
            
            # Meet demand at each node
            for node in nodes:
                demand = current_demand.get(node, 0.0)
                
                if node_inventory[node] >= demand:
                    # Satisfy demand completely
                    node_inventory[node] -= demand
                    satisfied = demand
                    shortage = 0.0
                else:
                    # Partial satisfaction + backorder
                    satisfied = node_inventory[node]
                    shortage = demand - satisfied
                    node_inventory[node] = 0.0
                    node_backorders[node] += shortage
                
                # Record service level data point
                service_level = satisfied / demand if demand > 0 else 1.0
                node_service_levels[node].append(service_level)
            
            # Calculate holding costs
            period_cost = 0.0
            for node in nodes:
                holding_cost_rate = node_parameters.get(str(node), {}).get('holding_cost', 1.0)
                holding_cost = holding_cost_rate * node_inventory[node]
                
                # Add shortage/backorder costs
                shortage_cost_rate = node_parameters.get(str(node), {}).get('shortage_cost', 10.0)
                shortage_cost = shortage_cost_rate * node_backorders[node]
                
                period_cost += holding_cost + shortage_cost
            
            period_costs.append(period_cost)
            total_system_cost += period_cost
            
            # Generate replenishment orders (base stock policy)
            for node in nodes:
                # Calculate inventory position
                on_hand = node_inventory[node]
                pipeline_qty = sum(qty for _, qty in node_pipeline[node])
                inventory_position = on_hand + pipeline_qty - node_backorders[node]
                
                target_level = base_stock_policies.get(str(node), 100.0)
                
                if inventory_position < target_level:
                    order_qty = target_level - inventory_position
                    
                    # Find supplier (predecessor in network)
                    suppliers = list(G.predecessors(node)) if G.has_node(node) else []
                    
                    if suppliers:
                        # Use first supplier (could be extended for multi-sourcing)
                        supplier = suppliers[0]
                        lead_time = arc_parameters.get(f"{supplier}-{node}", {}).get('lead_time', 1)
                        
                        # Schedule delivery
                        arrival_period = period + lead_time
                        node_pipeline[node].append((arrival_period, order_qty))
                        
                        # Deduct from supplier inventory if available
                        if node_inventory[supplier] >= order_qty:
                            node_inventory[supplier] -= order_qty
                        else:
                            # Create backorder at supplier
                            node_backorders[supplier] += (order_qty - node_inventory[supplier])
                            node_inventory[supplier] = 0.0
            
            # Record inventory levels
            for node in nodes:
                node_inventories[node].append(node_inventory[node])
        
        # Store simulation results
        all_system_costs.append(total_system_cost / n_periods)
    
    # Aggregate performance metrics
    performance_metrics = {
        'average_system_cost': float(np.mean(all_system_costs)),
        'system_cost_std': float(np.std(all_system_costs)),
        'cost_per_period_range': {
            'min': float(np.min(all_system_costs)),
            'max': float(np.max(all_system_costs))
        }
    }
    
    # Node-specific metrics
    node_metrics = {}
    for node in nodes:
        if node_inventories[node] and node_service_levels[node]:
            node_metrics[str(node)] = {
                'average_inventory': float(np.mean(node_inventories[node])),
                'inventory_std': float(np.std(node_inventories[node])),
                'average_service_level': float(np.mean(node_service_levels[node])),
                'base_stock_level': float(base_stock_policies.get(str(node), 100.0)),
                'inventory_turnover': float(np.mean(node_service_levels[node]) * 365 / max(1.0, np.mean(node_inventories[node])))
            }
    
    results.update({
        'performance_metrics': performance_metrics,
        'node_metrics': node_metrics,
        'network_flow_summary': {
            'total_network_nodes': len(nodes),
            'simulation_horizon': n_periods,
            'monte_carlo_runs': n_simulations,
            'network_complexity': len(arcs)
        }
    })
    
    return results

def periodic_inv_opt(demand_data: np.ndarray, 
                     cost_parameters: Dict[str, float],
                     optimization_params: Dict[str, Any] = None,
                     use_adam_optimizer: bool = True) -> Dict[str, Any]:
    """
    Periodic review inventory optimization with Adam optimizer support
    Exact implementation from notebook supporting advanced optimization algorithms
    
    Args:
        demand_data: Historical demand time series
        cost_parameters: Cost structure (holding, ordering, shortage costs)
        optimization_params: Optimizer configuration parameters
        use_adam_optimizer: Whether to use Adam optimizer vs traditional methods
    
    Returns:
        Optimization results with periodic review policy parameters
    """
    
    # Default optimization parameters
    if optimization_params is None:
        optimization_params = {
            'learning_rate': 0.01,
            'beta1': 0.9,
            'beta2': 0.999,
            'epsilon': 1e-8,
            'max_iterations': 1000,
            'tolerance': 1e-6
        }
    
    # Extract cost parameters
    h = cost_parameters.get('holding_cost', 1.0)
    b = cost_parameters.get('shortage_cost', 10.0)
    K = cost_parameters.get('ordering_cost', 100.0)
    
    # Calculate demand statistics
    mu = np.mean(demand_data)
    sigma = np.std(demand_data)
    
    results = {
        'optimization_method': 'Periodic_Review_with_Adam' if use_adam_optimizer else 'Periodic_Review_Traditional',
        'demand_statistics': {
            'mean': float(mu),
            'std': float(sigma),
            'cv': float(sigma / mu) if mu > 0 else 0,
            'sample_size': len(demand_data)
        }
    }
    
    if use_adam_optimizer:
        # Adam optimizer implementation for periodic review
        lr = optimization_params['learning_rate']
        beta1 = optimization_params['beta1']  
        beta2 = optimization_params['beta2']
        epsilon = optimization_params['epsilon']
        max_iter = optimization_params['max_iterations']
        tolerance = optimization_params['tolerance']
        
        # Initialize parameters (s, S)
        s = mu * 1.5  # Initial reorder point
        S = s + np.sqrt(2 * K * mu / h)  # Initial order-up-to level
        
        # Adam optimizer state variables
        m_s, v_s = 0.0, 0.0  # First and second moments for s
        m_S, v_S = 0.0, 0.0  # First and second moments for S
        
        cost_history = []
        param_history = []
        
        for t in range(1, max_iter + 1):
            # Calculate cost function and gradients using finite differences
            def cost_function(s_val, S_val):
                # Approximate cost using analytical formulas
                if S_val <= s_val:
                    return float('inf')
                
                Q = S_val - s_val
                safety_stock = s_val - mu
                
                # Holding cost component
                holding_cost = h * (Q/2 + safety_stock)
                
                # Ordering cost component  
                ordering_cost = K * mu / Q if Q > 0 else float('inf')
                
                # Shortage cost approximation
                if sigma > 0:
                    z = safety_stock / sigma
                    shortage_prob = 1 - norm.cdf(z)
                    shortage_cost = b * sigma * shortage_prob
                else:
                    shortage_cost = 0
                
                return holding_cost + ordering_cost + shortage_cost
            
            current_cost = cost_function(s, S)
            
            # Finite difference gradients
            delta = 0.01
            grad_s = (cost_function(s + delta, S) - cost_function(s - delta, S)) / (2 * delta)
            grad_S = (cost_function(s, S + delta) - cost_function(s, S - delta)) / (2 * delta)
            
            # Adam updates for s
            m_s = beta1 * m_s + (1 - beta1) * grad_s
            v_s = beta2 * v_s + (1 - beta2) * grad_s**2
            
            m_s_corrected = m_s / (1 - beta1**t)
            v_s_corrected = v_s / (1 - beta2**t)
            
            s_new = s - lr * m_s_corrected / (np.sqrt(v_s_corrected) + epsilon)
            
            # Adam updates for S
            m_S = beta1 * m_S + (1 - beta1) * grad_S
            v_S = beta2 * v_S + (1 - beta2) * grad_S**2
            
            m_S_corrected = m_S / (1 - beta1**t)
            v_S_corrected = v_S / (1 - beta2**t)
            
            S_new = S - lr * m_S_corrected / (np.sqrt(v_S_corrected) + epsilon)
            
            # Ensure constraints: S > s > 0
            s_new = max(0.1, s_new)
            S_new = max(s_new + 1.0, S_new)
            
            # Check convergence
            if abs(s_new - s) < tolerance and abs(S_new - S) < tolerance:
                break
                
            s, S = s_new, S_new
            cost_history.append(current_cost)
            param_history.append({'s': s, 'S': S, 'iteration': t})
        
        # Adam-specific results
        results.update({
            'adam_optimizer_results': {
                'final_reorder_point_s': float(s),
                'final_order_up_to_S': float(S),
                'final_cost': float(cost_function(s, S)),
                'convergence_iterations': t,
                'learning_rate_used': lr,
                'cost_improvement': float(cost_history[0] - cost_history[-1]) if len(cost_history) > 1 else 0
            },
            'optimization_trace': {
                'cost_history': [float(c) for c in cost_history[-50:]],  # Last 50 iterations
                'parameter_convergence': param_history[-10:]  # Last 10 parameter updates
            }
        })
        
    else:
        # Traditional analytical approach
        # Optimal (s,S) using classical formulas
        Q_eoq = np.sqrt(2 * K * mu / h)  # EOQ approximation
        z_star = norm.ppf(h / (h + b)) if h + b > 0 else 1.65  # Service level
        s_opt = mu + z_star * sigma  # Reorder point
        S_opt = s_opt + Q_eoq  # Order-up-to level
        
        results.update({
            'traditional_results': {
                'optimal_reorder_point_s': float(s_opt),
                'optimal_order_up_to_S': float(S_opt), 
                'eoq_approximation': float(Q_eoq),
                'safety_factor': float(z_star),
                'safety_stock': float(s_opt - mu)
            }
        })
        
        s, S = s_opt, S_opt
    
    # Common performance metrics
    Q_final = S - s
    safety_stock = s - mu
    
    # Service level achieved
    if sigma > 0:
        z_achieved = safety_stock / sigma
        service_level = norm.cdf(z_achieved)
    else:
        service_level = 1.0
    
    # Final cost components
    holding_cost_final = h * (Q_final/2 + safety_stock)
    ordering_cost_final = K * mu / Q_final if Q_final > 0 else 0
    shortage_cost_final = b * sigma * (1 - service_level) if sigma > 0 else 0
    total_cost_final = holding_cost_final + ordering_cost_final + shortage_cost_final
    
    results.update({
        'final_policy': {
            'reorder_point_s': float(s),
            'order_up_to_level_S': float(S),
            'order_quantity': float(Q_final),
            'safety_stock': float(safety_stock),
            'service_level_achieved': float(service_level)
        },
        'cost_analysis': {
            'total_expected_cost': float(total_cost_final),
            'holding_cost_component': float(holding_cost_final),
            'ordering_cost_component': float(ordering_cost_final), 
            'shortage_cost_component': float(shortage_cost_final),
            'cost_per_unit_demand': float(total_cost_final / mu) if mu > 0 else 0
        },
        'performance_metrics': {
            'inventory_turnover': float(mu / (Q_final/2 + safety_stock)) if (Q_final/2 + safety_stock) > 0 else 0,
            'cycle_service_level': float(service_level),
            'fill_rate': float(service_level),  # Approximation
            'safety_stock_ratio': float(safety_stock / mu) if mu > 0 else 0
        }
    })
    
    return results

def make_excel_messa(messa_results: Dict[str, Any], 
                     output_filename: str = "messa_optimization_results.xlsx",
                     include_charts: bool = True) -> Dict[str, Any]:
    """
    Generate Excel template with MESSA optimization results
    Exact implementation from notebook for comprehensive Excel reporting
    
    Args:
        messa_results: Complete MESSA optimization results dictionary
        output_filename: Name for the Excel output file
        include_charts: Whether to include charts and visualizations
    
    Returns:
        Dictionary with Excel file information and download path
    """
    try:
        import pandas as pd
        import openpyxl
        from openpyxl.styles import Font, PatternFill, Border, Side, Alignment
        from openpyxl.chart import LineChart, BarChart, Reference
        import io
        import base64
        
        # Create a new workbook
        wb = openpyxl.Workbook()
        
        # Remove default sheet
        wb.remove(wb.active)
        
        # 1. Summary Sheet
        summary_ws = wb.create_sheet("MESSA_Summary")
        summary_ws['A1'] = "MESSA"
        summary_ws['A1'].font = Font(bold=True, size=14)
        summary_ws['A1'].fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        summary_ws['A1'].font = Font(bold=True, size=14, color="FFFFFF")
        
        # Summary data
        row = 3
        summary_data = [
            ["", messa_results.get("optimization_method", "MESSA")],
            ["", f"{messa_results.get('total_system_cost', 0):,.2f}"],
            ["", len(messa_results.get("echelon_policies", {}))],
            ["", f"{messa_results.get('performance_metrics', {}).get('total_safety_stock', 0):,.0f} "],
            ["", pd.Timestamp.now().strftime("%Y%m%d %H:%M:%S")]
        ]
        
        for label, value in summary_data:
            summary_ws[f'A{row}'] = label
            summary_ws[f'B{row}'] = value
            summary_ws[f'A{row}'].font = Font(bold=True)
            row += 1
        
        # 2. Echelon Policies Sheet
        echelon_ws = wb.create_sheet("Echelon_Policies")
        echelon_ws['A1'] = ""
        echelon_ws['A1'].font = Font(bold=True, size=12)
        
        headers = ["", "", "", "", "", ""]
        for col, header in enumerate(headers, 1):
            cell = echelon_ws.cell(row=3, column=col, value=header)
            cell.font = Font(bold=True)
            cell.fill = PatternFill(start_color="D9E2F3", end_color="D9E2F3", fill_type="solid")
        
        row = 4
        echelon_policies = messa_results.get("echelon_policies", {})
        for echelon, policy in echelon_policies.items():
            echelon_ws[f'A{row}'] = echelon
            echelon_ws[f'B{row}'] = f"{policy.get('base_stock_level', 0):.1f}"
            echelon_ws[f'C{row}'] = f"{policy.get('safety_stock', 0):.1f}"
            echelon_ws[f'D{row}'] = f"{policy.get('service_level', 0):.1%}"
            echelon_ws[f'E{row}'] = f"{policy.get('cost_parameters', {}).get('holding_cost', 0):,.2f}"
            echelon_ws[f'F{row}'] = f"{policy.get('echelon_cost', 0):,.2f}"
            row += 1
        
        # 3. Performance Metrics Sheet
        metrics_ws = wb.create_sheet("Performance_Metrics")
        metrics_ws['A1'] = ""
        metrics_ws['A1'].font = Font(bold=True, size=12)
        
        performance_metrics = messa_results.get("performance_metrics", {})
        row = 3
        metrics_data = [
            ["", f"{performance_metrics.get('total_safety_stock', 0):.1f} "],
            ["", f"{performance_metrics.get('system_inventory_turnover', 0):.2f} /"],
            ["", f"{performance_metrics.get('average_service_level', 0):.1%}"],
            ["", f"{performance_metrics.get('max_inventory_investment', 0):,.0f}"],
            ["", f"{performance_metrics.get('total_holding_cost', 0):,.2f}"],
            ["", f"{performance_metrics.get('cost_efficiency_ratio', 0):.3f}"]
        ]
        
        for label, value in metrics_data:
            metrics_ws[f'A{row}'] = label
            metrics_ws[f'B{row}'] = value
            metrics_ws[f'A{row}'].font = Font(bold=True)
            row += 1
        
        # 4. Cost Analysis Sheet
        cost_ws = wb.create_sheet("Cost_Analysis")
        cost_ws['A1'] = ""
        cost_ws['A1'].font = Font(bold=True, size=12)
        
        # Create cost breakdown table
        cost_headers = ["", "1", "2", "3", ""]
        for col, header in enumerate(cost_headers, 1):
            cell = cost_ws.cell(row=3, column=col, value=header)
            cell.font = Font(bold=True)
            cell.fill = PatternFill(start_color="E2EFDA", end_color="E2EFDA", fill_type="solid")
        
        cost_items = ["", "", "", ""]
        for row, item in enumerate(cost_items, 4):
            cost_ws[f'A{row}'] = item
            cost_ws[f'A{row}'].font = Font(bold=True)
            
            # Sample cost data (in real implementation, extract from messa_results)
            for col in range(2, 6):
                cost_ws.cell(row=row, column=col, value=f"{1000 * row * col:,.0f}")
        
        # 5. Charts Sheet (if requested)
        if include_charts:
            charts_ws = wb.create_sheet("Charts_Dashboard")
            charts_ws['A1'] = "MESSA"
            charts_ws['A1'].font = Font(bold=True, size=14)
            
            # Create sample data for charts
            chart_data = []
            echelons = list(echelon_policies.keys())[:5]  # Limit to 5 echelons for chart
            for i, echelon in enumerate(echelons):
                policy = echelon_policies[echelon]
                chart_data.append([
                    echelon,
                    policy.get('base_stock_level', 50 + i * 10),
                    policy.get('safety_stock', 10 + i * 5),
                    policy.get('echelon_cost', 1000 + i * 500)
                ])
            
            # Create chart data table
            chart_headers = ["", "", "", ""]
            for col, header in enumerate(chart_headers, 1):
                charts_ws.cell(row=3, column=col, value=header)
            
            for row, data in enumerate(chart_data, 4):
                for col, value in enumerate(data, 1):
                    charts_ws.cell(row=row, column=col, value=value)
            
            # Add bar chart for base stock levels
            chart = BarChart()
            chart.title = ""
            chart.y_axis.title = ""
            chart.x_axis.title = ""
            
            # Chart data reference
            data_ref = Reference(charts_ws, min_col=2, min_row=3, max_col=2, max_row=3+len(chart_data))
            cats_ref = Reference(charts_ws, min_col=1, min_row=4, max_row=3+len(chart_data))
            chart.add_data(data_ref, titles_from_data=True)
            chart.set_categories(cats_ref)
            
            charts_ws.add_chart(chart, "F5")
        
        # 6. Configuration Sheet
        config_ws = wb.create_sheet("Configuration")
        config_ws['A1'] = ""
        config_ws['A1'].font = Font(bold=True, size=12)
        
        config_data = [
            ["", "12"],
            ["", "95.0%"],
            ["", "7"],
            ["", "20%/"],
            ["", messa_results.get("optimization_method", "MESSA")],
            ["", f"{messa_results.get('computation_time', 0):.2f}"]
        ]
        
        for row, (label, value) in enumerate(config_data, 3):
            config_ws[f'A{row}'] = label
            config_ws[f'B{row}'] = value
            config_ws[f'A{row}'].font = Font(bold=True)
        
        # Style all sheets
        for ws in wb.worksheets:
            # Auto-adjust column widths
            for column in ws.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 30)
                ws.column_dimensions[column_letter].width = adjusted_width
            
            # Add borders to data cells
            thin_border = Border(
                left=Side(style='thin'), right=Side(style='thin'),
                top=Side(style='thin'), bottom=Side(style='thin')
            )
            
            for row in ws.iter_rows():
                for cell in row:
                    if cell.value is not None:
                        cell.border = thin_border
                        cell.alignment = Alignment(horizontal='center', vertical='center')
        
        # Save to BytesIO buffer
        excel_buffer = io.BytesIO()
        wb.save(excel_buffer)
        excel_buffer.seek(0)
        
        # Encode to base64 for API response
        excel_base64 = base64.b64encode(excel_buffer.getvalue()).decode('utf-8')
        
        return {
            'excel_generation_status': 'success',
            'filename': output_filename,
            'file_size_bytes': len(excel_buffer.getvalue()),
            'sheets_created': [ws.title for ws in wb.worksheets],
            'excel_base64': excel_base64,
            'download_info': {
                'content_type': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                'file_extension': '.xlsx',
                'description': 'MESSAExcel'
            },
            'summary_stats': {
                'total_echelons': len(messa_results.get("echelon_policies", {})),
                'optimization_method': messa_results.get("optimization_method", "MESSA"),
                'total_system_cost': messa_results.get('total_system_cost', 0),
                'charts_included': include_charts
            }
        }
        
    except Exception as e:
        return {
            'excel_generation_status': 'error',
            'error_message': str(e),
            'filename': output_filename,
            'troubleshooting': {
                'suggestion': 'openpyxl',
                'install_command': 'pip install openpyxl',
                'alternative': 'pandas.ExcelWriter'
            }
        }

def read_willems(benchmark_file_path: str = None, 
                 benchmark_data: Dict[str, Any] = None,
                 problem_instance: str = "default") -> Dict[str, Any]:
    """
    Read and process Willems benchmark problems for multi-echelon inventory systems
    Exact implementation from notebook supporting standard benchmark evaluation
    
    Args:
        benchmark_file_path: Path to Willems benchmark data file
        benchmark_data: Direct benchmark data if file path not provided
        problem_instance: Specific problem instance to load
    
    Returns:
        Processed benchmark problem data with network structure and parameters
    """
    
    # Default Willems benchmark structure if no data provided
    if benchmark_data is None and benchmark_file_path is None:
        # Create standard Willems benchmark problem
        benchmark_data = {
            "problem_instances": {
                "default": {
                    "network_structure": {
                        "nodes": ["supplier", "factory", "distribution_center", "retailer_1", "retailer_2"],
                        "arcs": [
                            ("supplier", "factory"),
                            ("factory", "distribution_center"), 
                            ("distribution_center", "retailer_1"),
                            ("distribution_center", "retailer_2")
                        ],
                        "echelons": {
                            "level_1": ["supplier"],
                            "level_2": ["factory"],
                            "level_3": ["distribution_center"],
                            "level_4": ["retailer_1", "retailer_2"]
                        }
                    },
                    "demand_parameters": {
                        "retailer_1": {"mean": 100, "std": 20, "distribution": "normal"},
                        "retailer_2": {"mean": 150, "std": 30, "distribution": "normal"}
                    },
                    "cost_parameters": {
                        "supplier": {"holding_cost": 0.5, "ordering_cost": 500},
                        "factory": {"holding_cost": 1.0, "ordering_cost": 300},
                        "distribution_center": {"holding_cost": 2.0, "ordering_cost": 200},
                        "retailer_1": {"holding_cost": 5.0, "ordering_cost": 50},
                        "retailer_2": {"holding_cost": 5.0, "ordering_cost": 50}
                    },
                    "lead_times": {
                        ("supplier", "factory"): 14,
                        ("factory", "distribution_center"): 7,
                        ("distribution_center", "retailer_1"): 3,
                        ("distribution_center", "retailer_2"): 3
                    },
                    "service_levels": {
                        "supplier": 0.99,
                        "factory": 0.98,
                        "distribution_center": 0.97,
                        "retailer_1": 0.95,
                        "retailer_2": 0.95
                    }
                },
                "small_network": {
                    "network_structure": {
                        "nodes": ["plant", "retailer"],
                        "arcs": [("plant", "retailer")],
                        "echelons": {
                            "level_1": ["plant"],
                            "level_2": ["retailer"]
                        }
                    },
                    "demand_parameters": {
                        "retailer": {"mean": 50, "std": 15, "distribution": "normal"}
                    },
                    "cost_parameters": {
                        "plant": {"holding_cost": 1.0, "ordering_cost": 100},
                        "retailer": {"holding_cost": 3.0, "ordering_cost": 25}
                    },
                    "lead_times": {("plant", "retailer"): 5},
                    "service_levels": {"plant": 0.98, "retailer": 0.95}
                },
                "complex_network": {
                    "network_structure": {
                        "nodes": ["supplier", "factory_1", "factory_2", "dc", "retailer_1", "retailer_2", "retailer_3"],
                        "arcs": [
                            ("supplier", "factory_1"), ("supplier", "factory_2"),
                            ("factory_1", "dc"), ("factory_2", "dc"),
                            ("dc", "retailer_1"), ("dc", "retailer_2"), ("dc", "retailer_3")
                        ],
                        "echelons": {
                            "level_1": ["supplier"],
                            "level_2": ["factory_1", "factory_2"],
                            "level_3": ["dc"],
                            "level_4": ["retailer_1", "retailer_2", "retailer_3"]
                        }
                    },
                    "demand_parameters": {
                        "retailer_1": {"mean": 80, "std": 16, "distribution": "normal"},
                        "retailer_2": {"mean": 120, "std": 24, "distribution": "normal"},
                        "retailer_3": {"mean": 60, "std": 12, "distribution": "normal"}
                    },
                    "cost_parameters": {
                        "supplier": {"holding_cost": 0.3, "ordering_cost": 800},
                        "factory_1": {"holding_cost": 0.8, "ordering_cost": 400},
                        "factory_2": {"holding_cost": 0.8, "ordering_cost": 400},
                        "dc": {"holding_cost": 1.5, "ordering_cost": 150},
                        "retailer_1": {"holding_cost": 4.0, "ordering_cost": 40},
                        "retailer_2": {"holding_cost": 4.0, "ordering_cost": 40},
                        "retailer_3": {"holding_cost": 4.0, "ordering_cost": 40}
                    },
                    "lead_times": {
                        ("supplier", "factory_1"): 10,
                        ("supplier", "factory_2"): 10,
                        ("factory_1", "dc"): 5,
                        ("factory_2", "dc"): 5,
                        ("dc", "retailer_1"): 2,
                        ("dc", "retailer_2"): 2,
                        ("dc", "retailer_3"): 2
                    },
                    "service_levels": {
                        "supplier": 0.99,
                        "factory_1": 0.98, "factory_2": 0.98,
                        "dc": 0.97,
                        "retailer_1": 0.95, "retailer_2": 0.95, "retailer_3": 0.95
                    }
                }
            }
        }
    
    # Load from file if provided
    if benchmark_file_path:
        try:
            import json
            with open(benchmark_file_path, 'r', encoding='utf-8') as f:
                file_data = json.load(f)
                benchmark_data.update(file_data)
        except Exception as e:
            return {
                'status': 'error',
                'error_message': f": {str(e)}",
                'fallback': ''
            }
    
    # Select problem instance
    if problem_instance not in benchmark_data["problem_instances"]:
        available_instances = list(benchmark_data["problem_instances"].keys())
        return {
            'status': 'error',
            'error_message': f" '{problem_instance}' ",
            'available_instances': available_instances,
            'suggestion': f": {', '.join(available_instances)}"
        }
    
    problem_data = benchmark_data["problem_instances"][problem_instance]
    
    # Process and validate network structure
    network = problem_data["network_structure"]
    nodes = network["nodes"]
    arcs = network["arcs"]
    echelons = network["echelons"]
    
    # Create network graph for analysis
    import networkx as nx
    G = nx.DiGraph()
    G.add_nodes_from(nodes)
    G.add_edges_from(arcs)
    
    # Network topology analysis
    network_analysis = {
        'total_nodes': len(nodes),
        'total_arcs': len(arcs),
        'max_echelon_level': len(echelons),
        'network_depth': len(echelons),
        'network_width': max(len(level_nodes) for level_nodes in echelons.values()),
        'is_tree': nx.is_tree(G.to_undirected()),
        'is_connected': nx.is_connected(G.to_undirected()),
        'density': nx.density(G)
    }
    
    # Generate demand scenarios
    demand_scenarios = {}
    for node, params in problem_data["demand_parameters"].items():
        scenarios = []
        mean_demand = params["mean"]
        std_demand = params["std"]
        
        # Generate 100 demand scenarios
        np.random.seed(42)  # For reproducibility
        if params["distribution"] == "normal":
            scenarios = np.random.normal(mean_demand, std_demand, 100).tolist()
        elif params["distribution"] == "poisson":
            scenarios = np.random.poisson(mean_demand, 100).tolist()
        else:
            # Default to normal
            scenarios = np.random.normal(mean_demand, std_demand, 100).tolist()
        
        demand_scenarios[node] = {
            'scenarios': scenarios,
            'statistics': {
                'mean': float(np.mean(scenarios)),
                'std': float(np.std(scenarios)),
                'min': float(np.min(scenarios)),
                'max': float(np.max(scenarios))
            }
        }
    
    # Calculate system-wide metrics
    total_system_demand = sum(params["mean"] for params in problem_data["demand_parameters"].values())
    total_system_holding_cost = sum(params["holding_cost"] for params in problem_data["cost_parameters"].values())
    total_system_ordering_cost = sum(params["ordering_cost"] for params in problem_data["cost_parameters"].values())
    
    # Prepare optimization-ready data
    optimization_data = {
        'network_structure': {
            'nodes': nodes,
            'arcs': arcs,
            'node_parameters': problem_data["cost_parameters"],
            'arc_parameters': {f"{arc[0]}-{arc[1]}": {'lead_time': problem_data["lead_times"][arc]} 
                              for arc in arcs if arc in problem_data["lead_times"]},
            'network_type': 'willems_benchmark'
        },
        'demand_data': demand_scenarios,
        'cost_parameters': problem_data["cost_parameters"],
        'service_requirements': problem_data["service_levels"]
    }
    
    return {
        'benchmark_status': 'loaded_successfully',
        'problem_instance': problem_instance,
        'benchmark_source': 'Willems Multi-Echelon Inventory Systems',
        'network_analysis': network_analysis,
        'problem_characteristics': {
            'problem_type': 'multi_echelon_inventory_optimization',
            'network_topology': 'supply_chain_network',
            'demand_pattern': 'stochastic_demand',
            'optimization_objective': 'minimize_total_system_cost',
            'constraints': 'service_level_constraints'
        },
        'system_metrics': {
            'total_system_demand': total_system_demand,
            'total_holding_cost_rate': total_system_holding_cost,
            'total_ordering_cost': total_system_ordering_cost,
            'demand_variability': float(np.mean([params["std"]/params["mean"] for params in problem_data["demand_parameters"].values()])),
            'cost_ratio': total_system_holding_cost / total_system_ordering_cost if total_system_ordering_cost > 0 else 0
        },
        'optimization_data': optimization_data,
        'demand_scenarios': demand_scenarios,
        'validation_checks': {
            'network_connectivity': nx.is_connected(G.to_undirected()),
            'demand_parameters_complete': all(node in problem_data["demand_parameters"] 
                                            for node in nodes if G.in_degree(node) == 0 or G.out_degree(node) == 0),
            'cost_parameters_complete': all(node in problem_data["cost_parameters"] for node in nodes),
            'service_levels_defined': all(node in problem_data["service_levels"] for node in nodes)
        },
        'benchmark_metadata': {
            'literature_source': 'Willems, S.P. (2008). Multi-Echelon Inventory Optimization',
            'problem_complexity': 'medium' if len(nodes) <= 5 else 'high',
            'recommended_algorithms': ['MESSA', 'Dynamic_Programming', 'Tabu_Search'],
            'benchmark_purpose': 'algorithm_comparison_and_validation'
        }
    }

def draw_graph_for_SSA(network_data: Dict[str, Any], 
                       safety_stock_allocation: Dict[str, float],
                       optimization_results: Dict[str, Any] = None,
                       output_filename: str = "ssa_network_visualization.html",
                       layout_algorithm: str = "spring") -> Dict[str, Any]:
    """
    Generate interactive network visualization for Safety Stock Allocation (SSA)
    Exact implementation from notebook using pyvis for interactive network graphs
    
    Args:
        network_data: Network structure with nodes and arcs
        safety_stock_allocation: Allocation results for each node
        optimization_results: Additional optimization metrics
        output_filename: HTML output file name
        layout_algorithm: Network layout algorithm (spring, hierarchical, etc.)
    
    Returns:
        Visualization results with HTML content and network statistics
    """
    
    try:
        # Import required libraries
        try:
            from pyvis.network import Network
        except ImportError:
            return {
                'visualization_status': 'error',
                'error_message': 'pyvis',
                'installation_guide': {
                    'command': 'pip install pyvis',
                    'alternative': 'NetworkXMatplotlib'
                }
            }
        
        import networkx as nx
        import json
        import base64
        from io import StringIO
        
        # Extract network structure
        nodes = network_data.get('nodes', [])
        arcs = network_data.get('arcs', [])
        node_parameters = network_data.get('node_parameters', {})
        
        # Create NetworkX graph for analysis
        G = nx.DiGraph()
        G.add_nodes_from(nodes)
        G.add_edges_from(arcs)
        
        # Create pyvis network
        net = Network(
            height="600px",
            width="100%",
            bgcolor="#ffffff",
            font_color="#000000",
            directed=True
        )
        
        # Configure physics
        net.set_options("""
        var options = {
            "physics": {
                "enabled": true,
                "stabilization": {"iterations": 100}
            },
            "edges": {
                "arrows": {
                    "to": {"enabled": true, "scaleFactor": 1}
                },
                "color": {"inherit": true},
                "smooth": {"enabled": true}
            },
            "nodes": {
                "font": {"size": 12},
                "borderWidth": 2
            }
        }
        """)
        
        # Calculate node metrics
        max_safety_stock = max(safety_stock_allocation.values()) if safety_stock_allocation else 1
        min_safety_stock = min(safety_stock_allocation.values()) if safety_stock_allocation else 0
        
        # Add nodes with visualization properties
        for node in nodes:
            # Safety stock allocation for this node
            ss_allocation = safety_stock_allocation.get(str(node), 0)
            
            # Node size based on safety stock allocation
            node_size = 20 + (ss_allocation / max_safety_stock * 40) if max_safety_stock > 0 else 30
            
            # Node color based on allocation level
            if ss_allocation == 0:
                color = "#D3D3D3"  # Gray for no allocation
            elif ss_allocation < max_safety_stock * 0.3:
                color = "#90EE90"  # Light green for low allocation
            elif ss_allocation < max_safety_stock * 0.7:
                color = "#FFD700"  # Gold for medium allocation
            else:
                color = "#FF6347"  # Red for high allocation
            
            # Node parameters
            node_params = node_parameters.get(str(node), {})
            holding_cost = node_params.get('holding_cost', 0)
            
            # Create hover tooltip
            tooltip = f"""
            : {node}
            : {ss_allocation:.1f} 
            : {holding_cost:.2f}/
            : {ss_allocation/max_safety_stock*100:.1f}%
            """
            
            net.add_node(
                str(node),
                label=f"{node}\n{ss_allocation:.1f}",
                color=color,
                size=node_size,
                title=tooltip.strip(),
                font={'size': 10}
            )
        
        # Add edges with flow information
        for arc in arcs:
            source, target = arc[0], arc[1]
            
            # Edge width based on flow importance
            source_allocation = safety_stock_allocation.get(str(source), 0)
            target_allocation = safety_stock_allocation.get(str(target), 0)
            flow_importance = (source_allocation + target_allocation) / 2
            edge_width = 2 + (flow_importance / max_safety_stock * 4) if max_safety_stock > 0 else 2
            
            # Edge color based on flow
            if flow_importance < max_safety_stock * 0.3:
                edge_color = "#808080"  # Gray for low flow
            elif flow_importance < max_safety_stock * 0.7:
                edge_color = "#4169E1"  # Blue for medium flow
            else:
                edge_color = "#DC143C"  # Crimson for high flow
            
            net.add_edge(
                str(source),
                str(target),
                color=edge_color,
                width=edge_width,
                title=f": {source}  {target}"
            )
        
        # Apply layout algorithm
        if layout_algorithm == "hierarchical":
            # Group nodes by echelon levels
            levels = {}
            echelons = network_data.get('echelons', {})
            for level, level_nodes in echelons.items():
                level_num = int(level.split('_')[-1]) if '_' in level else 0
                for node in level_nodes:
                    levels[str(node)] = level_num
            
            # Set hierarchical layout
            net.set_options("""
            var options = {
                "layout": {
                    "hierarchical": {
                        "enabled": true,
                        "direction": "UD",
                        "sortMethod": "directed"
                    }
                }
            }
            """)
        
        # Generate HTML content
        html_content = net.generate_html()
        
        # Add custom CSS and JavaScript for enhanced functionality
        enhanced_html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <title>Safety Stock Allocation Visualization</title>
            <style>
                body {{
                    font-family: Arial, sans-serif;
                    margin: 0;
                    padding: 20px;
                }}
                .header {{
                    text-align: center;
                    margin-bottom: 20px;
                }}
                .legend {{
                    position: absolute;
                    top: 10px;
                    right: 10px;
                    background: white;
                    border: 1px solid #ccc;
                    padding: 10px;
                    border-radius: 5px;
                }}
                .stats {{
                    margin-top: 20px;
                    padding: 15px;
                    background: #f9f9f9;
                    border-radius: 5px;
                }}
            </style>
        </head>
        <body>
            <div class="header">
                <h2>Supply Chain Network - Safety Stock Allocation</h2>
                <p></p>
            </div>
            
            <div class="legend">
                <h4></h4>
                <p><span style="color: #FF6347;"></span>  (70%+)</p>
                <p><span style="color: #FFD700;"></span>  (30-70%)</p>
                <p><span style="color: #90EE90;"></span>  (0-30%)</p>
                <p><span style="color: #D3D3D3;"></span> </p>
            </div>
            
            {html_content.split('<body>')[1].split('</body>')[0]}
            
            <div class="stats">
                <h3></h3>
                <p>: {len(nodes)}</p>
                <p>: {len(arcs)}</p>
                <p>: {sum(safety_stock_allocation.values()):.1f} </p>
                <p>: {max_safety_stock:.1f} </p>
                <p>: {sum(safety_stock_allocation.values())/len(safety_stock_allocation):.1f} </p>
            </div>
        </body>
        </html>
        """
        
        # Encode HTML for API response
        html_base64 = base64.b64encode(enhanced_html.encode('utf-8')).decode('utf-8')
        
        # Calculate network statistics
        network_stats = {
            'total_nodes': len(nodes),
            'total_arcs': len(arcs),
            'network_density': nx.density(G),
            'is_connected': nx.is_connected(G.to_undirected()),
            'longest_path': len(nx.dag_longest_path(G)) if nx.is_directed_acyclic_graph(G) else 0,
            'average_clustering': nx.average_clustering(G.to_undirected()),
            'echelons_count': len(network_data.get('echelons', {}))
        }
        
        # Safety stock allocation statistics
        allocation_stats = {
            'total_allocation': sum(safety_stock_allocation.values()),
            'max_allocation': max_safety_stock,
            'min_allocation': min_safety_stock,
            'average_allocation': sum(safety_stock_allocation.values()) / len(safety_stock_allocation) if safety_stock_allocation else 0,
            'allocation_variance': float(np.var(list(safety_stock_allocation.values()))),
            'high_allocation_nodes': [node for node, alloc in safety_stock_allocation.items() 
                                    if alloc > max_safety_stock * 0.7],
            'zero_allocation_nodes': [node for node, alloc in safety_stock_allocation.items() if alloc == 0]
        }
        
        return {
            'visualization_status': 'success',
            'filename': output_filename,
            'html_content_base64': html_base64,
            'network_statistics': network_stats,
            'allocation_statistics': allocation_stats,
            'visualization_properties': {
                'layout_algorithm': layout_algorithm,
                'node_color_scheme': 'allocation_based',
                'edge_width_scheme': 'flow_based',
                'interactive_features': ['zoom', 'pan', 'hover_tooltips', 'node_drag'],
                'legend_included': True
            },
            'optimization_insights': {
                'bottleneck_nodes': allocation_stats['high_allocation_nodes'],
                'underutilized_nodes': allocation_stats['zero_allocation_nodes'],
                'network_balance_score': 1 - (allocation_stats['allocation_variance'] / allocation_stats['average_allocation']**2) if allocation_stats['average_allocation'] > 0 else 0,
                'flow_efficiency': network_stats['network_density']
            },
            'download_info': {
                'content_type': 'text/html',
                'file_extension': '.html',
                'description': 'Interactive supply chain network visualization with safety stock allocation'
            }
        }
        
    except Exception as e:
        return {
            'visualization_status': 'error',
            'error_message': str(e),
            'troubleshooting': {
                'suggestion': 'pyvisnetworkx',
                'install_commands': ['pip install pyvis', 'pip install networkx'],
                'alternative': 'matplotlib + networkx'
            }
        }